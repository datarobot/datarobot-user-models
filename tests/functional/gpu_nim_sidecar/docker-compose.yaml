services:
  drum:
    image: ${DOCKER_IMAGE}
    entrypoint: ["./tests/functional/run_integration_tests_in_framework_container.sh", "nim_sidecar"]
    environment:
      - GPU_COUNT=${GPU_COUNT}  # not required by predictor for multi-container, but is required to not-skip tests
      - NGC_API_KEY=${NGC_API_KEY}
      - OPENAI_HOST=openai
      - TARGET_TYPE=textgeneration
      - TARGET_NAME=prompt
      - TEST_URL_HOST=${TEST_URL_HOST}
    working_dir: ${GIT_ROOT}
    volumes:
      - ${HOME}:${HOME}
      - ${GIT_ROOT}:${GIT_ROOT}
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp:/tmp
    ports:
      - 8080:8080
    networks:
      - my_network
    restart: no
    depends_on:
      - openai

  openai:
    image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.3.3
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
      - NIM_SERVED_MODEL_NAME=datarobot-deployed-llm  # as hardcoded in NIM Predictor
    ports:
      - 8000:8000
    networks:
      - my_network
    restart: no
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  my_network:
    driver: bridge