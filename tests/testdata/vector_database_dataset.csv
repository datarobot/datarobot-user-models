relevant
"['---\ntitle: Edit and use applications\ndescription: Configure, share, and present AI-powered apps in Workbench.\nsection_name: Workbench\n---\n\n# Edit and use applications {: #edit-and-use-applications }\n\nWorkbench applications have two modes: **Edit** mode, where you can configure, customize, and even use the application, and **Present** mode, an end-user view of the application.\n\n!!! note\n     The instructions on this page demonstrate how to edit and use an application in Edit mode.\n\nThe tabs below describe each mode in more detail, including the purpose and limitations of each one:\n\n=== ""Edit mode""\n\n     To enter Edit mode, when in Present mode, click **Edit** in the upper-right corner. In this mode, you can:\n\n     - Customize the appearance of the application.\n     - Share applications.\n     - Configure widgets and customize the appearance of widget visualizations.\n     - Make predictions and interact with prediction results.\n     - View summary information and model insights.\n     - Return to your Use Case via the breadcrumbs at the top of the page.\n\n     See the image and table below for a brief description of the interface:\n\n     ![](images/wb-app-edit-1.png)\n\n     &nbsp;      |         Description\n     ----------- | ---------------------\n     ![](images/icon-wb-1.png)  | Organizes the application using [folders and sections](#application-folders).\n     ![](images/icon-wb-2.png)  | Displays the selected folder and its sections (indicated by the open folder on the left).         \n     ![](images/icon-wb-3.png)  |   **Share** provides a shareable link that grants recipients access to the application. <br><br>**Present** opens the end-user version of the application. If you are already presenting, DataRobot displays an **Edit** button.           \n\n=== ""Present mode""\n\n     To enter Present mode, when in Edit mode, click **Present** in the upper-right corner to enter Present mode. Present mode displays the end-user version of the application that anonymous users (accessing the app via the shareable link) and those with `Consumer` access can access. In this mode, you can:\n\n     - Share applications.\n     - Make predictions and interact with prediction results.\n     - View summary information and model insights.\n\n     ![](images/wb-app-present.png)\n\n     \n## Change themes {: #change-themes }\n\nIn the **Themes** tab, you can choose a light or dark theme for your application.\n\n![](images/wb-app-theme.png)\n\n## Upload a custom logo {: #upload-a-custom-logo }\n\nTo add a custom logo to your application, click **Upload Logo** and select the image you\'d like to use.\n\n![](images/wb-app-logo.png)\n\n## Add sections {: #add-sections }\n\nAlthough you can\'t remove default sections from your application, you can add a new section to any of the folders. These custom sections include an editable text field and heading that when edited, also updates the section name in the left panel.\n\n![](images/wb-app-section.png)\n\nTo remove a custom section, click the trash icon.\n\n![](images/wb-app-section-1.png)\n\n## Reorder sections {: #reorder-sections }\n\nYou can change the order of where sections appear within a folder by dragging them to a new position in the left panel.\n\n![](images/wb-app-reorder.gif)\n\n## Customize visualizations {: #customize-visualizations }\n\nIf a section contains a visualization, you can customize its appearance and behavior. To customize a visualization, hover over the chart and click the pencil icon.\n\n![](images/wb-app-viz-2.png)\n\nIn the below example, you can change the chart colors, number of bins, sorting order, and partition source. However, the available customization options vary by visualization.\n\n![](images/wb-app-viz-1.png)\n\n## Application folders {: #application-folders }\n\nBy default, DataRobot automatically organizes your application by intuitively grouping its content in folders, and within each folder, information, insights, and actions are divided into sections. Default folders and sections cannot be removed, however, you can [add new sections](#add-sections) to a folder. Each application contains the following folders:\n\n   Folder       |       Description\n ----------------- | ---------------------\n  [Overview](#overview)       |   Displays general summary information.       \n  [Insights](#insights)      |  Displays various model insights.        \n [Predictions](#predictions)     |  Allows you to make and view predictions.        \n [Prediction Details](#prediction-details)     |  Displays individual prediction results. \n\n### Overview  {: #overview } \n\nThe **Overview** folder contains general summary information for your application.\n\n![](images/wb-app-edit-2.png)\n\nSection       |  Type   |     Description\n----------------- |  --------  | ---------------------\nUse Case summary       |  Text  | Displays summary information for the application\'s Use Case.       \nProblem statement/Why it\'s valuable   |  Text    |  Allows you to enter a problem statement and description of application value.        \nExperiment summary     |  Text  | Displays summary information for the application\'s experiment.  \nBlueprint Chart     |  Visualization  | Displays the blueprint of the model used to create the application. <br>See the [full documentation](blueprints){ target=_blank }.   \nLift chart     |  Visualization  |  Depicts how well a model segments the target population and how capable it is of predicting the target. <br>See the [full documentation](lift-chart){ target=_blank }.    \n\n### Insights {: #insights }\n\nThe **Insights** folder displays several insights (if available) for the original model that was used to create the application.\n\n![](images/wb-app-edit-3.png)\n\nSection   | Type |   Description    \n--------  |  -------  |  -------------    \nFeature Impact  |  Visualization  | Provides a high-level visualization that identifies which features are most strongly driving model decisions. <br>See the [full documentation](feature-impact){ target=_blank }.\nROC Curve | Visualization |  Provides visualizations and metrics to help you determine whether the classification performance of a particular model meets your specifications. Only available for binary classification models. <br>See the [full documentation](roc-curve){ target=_blank }.\nFeature Effects | Visualization  |Visualizes the effect of changes in the value of each feature on the model’s predictions. <br>See the [full documentation](feature-effects){ target=_blank }.\nWord Cloud      | Visualization | Displays the most relevant words and short phrases in word cloud format. <br>See the [full documentation](analyze-insights#word-clouds){ target=_blank }.\nPrediction Explanations | Visualization |  Illustrates what drives predictions on a row-by-row basis—they provide a quantitative indicator of the effect variables have on the predictions, answering why a given model made a certain prediction. <br>See the [full documentation](predex-overview){ target=_blank }.\n\n### Predictions {: #predictions }\n\nThe **Predictions** folder allows you to make single record and batch predictions, as well as, view a history of predictions made in the application.\n\n![](images/wb-app-edit-4.png)\n\nSection       |  Type   |     Description\n----------------- |  --------  | ---------------------\nAll Rows       |  Visualization  | Displays each prediction row generated by the application.       \nSingle prediction   |  Action    | Allows you to submit single record predictions.         \nBatch prediction     |  Action  | Allows you to upload batch prediction files.  \n\n#### Make predictions {: #make-predictions }\n\nTo learn how to make both single and batch predictions in an application, see the existing documentation on [making applications](app-make-pred).\n\n### Prediction Details {: #prediction-details }\n\nThe **Prediction Details** folder displays prediction results for individual predictions. Note that this folder is disabled until the app is used to make a prediction.\n\n![](images/wb-app-edit-5.png)\n\nSection       |  Type   |     Description\n----------------- |  --------  | ---------------------\nGeneral Information       |  Text  | Displays prediction information for the selected row.       \nPrediction Explanations   |  Visualization    | Displays Prediction Explanations for the selected row.         \nWhat-if and Optimizer     |  Visualization / Action  | Allows you to interact with prediction results using scenario comparison and optimizer tools.\n\nFor more information, see the documentation on [app prediction results](app-analyze-result) and the [What-if and Optimizer widget](whatif-opt).\n\n!!! note ""Enabling the what-if and optimizer tools""\n     By default, only the what-if tool is enabled in the What-if and Optimizer widget. To enable the optimizer tool and/or disable the what-if tool, make sure you\'re in **Edit** mode. Hover over the widget and click the **pencil** icon. Open the **Properties** tab and use the toggles to customize the widget\'s functionality.\n\n     ![](images/wb-whatif-1.png)\n\n#### Create what-if scenarios {: #create-what-if-scenarios }\n\nA decision-support tool that allows you to create and compare multiple prediction simulations to identify the option with the best outcome. You can also make a prediction, then change one or more inputs to create a new simulation, and see how those changes affect the target feature.\n\nLearn more about [using](app-analyze-result#create-simulations) and [configuring](whatif-opt) the what-if tool.\n\n#### Optimize predictions {: #optimize-predictions }\n\nIdentifies the maximum or minimum predicted value for a target or custom expression by varying the values of a selection of flexible features in the model.\n\n!!! note\n     To view optimizations, you must open **Present** mode.\n\nLearn more about [using](app-analyze-result#optimization-simulation-results) and [configuring](whatif-opt) the optimizer tool.\n\n\n## Share {: #share }\n\nThere are two ways to share applications:\n\n1. [Send a shareable link](#generate-a-shareable-link) to DataRobot or non-DataRobot users.\n2. [Share the Use Case](wb-build-usecase#share) with other DataRobot users, which provides them with access to all assets contained within it.\n\n### Generate a shareable link {: #with-a-shareable-link }\n\nWhen using the application&mdash;in either **Edit** and **Present** mode&mdash;you can generate a shareable link. This link provides access to your application and can be shared with non-DataRobot users.\n\nTo generate a shareable link: \n\n1. Click **Share** in the upper-right corner.\n\n     ![](images/wb-app-share.png)\n\n2. Select the box next to **Grant access to anyone with this link**. If this box does not have a check mark, users cannot access the application via the link.\n\n     ![](images/wb-app-share-1.png)\n\n3. (Optional) Toggle **Prevent users with link access from making predictions** to control the ability for recipients to make predictions in the app.\n\n!!! warning ""Generate a new link""\n     If you click **Generate a new link**, all users with the older link will no longer have access to the app&mdash; you must send a new link to grant access.\n\n', '---\ntitle: User Activity Monitor reference\ndescription:  Use the User Activity Monitor to preview or download the Admin Usage and Prediction Usage reports. You can filter, hide sensitive fields, export CSV, and more.\n\n---\n\n# User Activity Monitor reference {: #user-activity-monitor-reference }\n\n\nThe following sections describe the fields returned for the **User Activity Monitor** (UAM), based on the selected report view. See the [**User Activity Monitor** overview](main-uam-overview) for information on using the tool.\n\nEach row of both the online preview and download of reports relates to a single audited event. Data is presented in ascending order, with the earliest data appearing at the top of the report. You can download the reports using [Export CSV](main-uam-overview#download-activity-data). When exporting reports, you are prompted to filter records for report download. The filters you apply when previewing the report apply only to the online preview.\n\n## Hide sensitive information {: #hide-sensitive-information }\n\nWhen viewing App Usage or Prediction Usage reports, you can hide or display identifying information with the ""Include identifying fields"" option. You may want to hide the information, for example, if Customer Support will be accessing a report. If unchecked, the columns display in the report without values. The fields considered sensitive are marked with an asterisk in the tables below.\n\n## Admin Usage activity report {: #admin-usage-activity-report }\n\nThe Admin Usage activity output reports the following data about administrator operations and activities.\n\n|  Report field  |     Description    |\n|----------------|--------------------|\n| Timestamp—UTC  | Timestamp (UTC time standard) when the administrator event occurred |\n| Event          | Type of administrator event, such as Create Account, Organization created, Change Password, Update Account, etc.      |\n| UID            | ID for this user     |\n| Username (*)   | Username for this user       |\n| Admin Org ID   | ID of the administrator organization    |\n| Admin Org Name | Name of the administrator\'s organization  |\n| Org ID         | ID of the user\'s organization  |\n| Org Name (*)   | Name of the user\'s organization    |\n| Group ID       | ID for this user\'s group     |\n| Group Name     | Name of the user\'s group   |\n| Admin UID      | ID for this administrator (if applicable)   |\n| Admin Username | Username for this administrator (if applicable)     |\n| Old values (*) | User account settings before the administrator made changes. For example, if the administrator activity changed the workers for the related user, this field shows the ""max\\_workers"" value *before* the change. |\n| New values (*) | User account settings after the administrator made changes. For example, if the administrator activity changed the workers for the related user, this field shows the ""max\\_workers"" value *after* the change.   |\n\n\n(\\*) denotes an [identifying field](#hide-sensitive-information) for this report\n\n## App Usage activity report {: #app-usage-activity-report }\n\nThe App Usage activity output reports the following data about application events.\n\n|  Report field  |     Description    |\n|----------------|--------------------|\n| Timestamp—UTC       | Timestamp (UTC time standard) when the application event occurred   |\n| Event               | Type of application event, such as Add Model, Compliance Doc Generated, aiAPI Portal Login, Dataset Upload, etc.    |\n| UID                 | ID of the user   |\n| Username (*)        | Name of the user   |\n| Project ID          | ID of the project   |\n| Project Name (*)    | Name of the project    |\n| Org ID              | ID of the user\'s organization   |\n| Org Name (*)        | Name of the user\'s organization     |\n| Group ID            | ID of the user\'s group    |\n| Group Name (*)      | Name of the user\'s group   |\n| User Role           | Role for the user who initiated the event; values include OWNER, USER, OBSERVER     |\n| Project Type        | Type for the related project; possible values include Binary Classification, Regression, Time Series—Regression, Multiclass Classification, etc.    |\n| Metric              | Optimization metric for the related project; potential values include LogLoss, RMSE, AUC, etc.   |\n| Partition Method    | Partition method for the related project (i.e., how data is partitioned for this project)    |\n| Target Variable (\\*) | Target variable for the related project (i.e., what DataRobot will predict)   |\n| Model ID            | ID of the model    |\n| Model Type          | Type for the model; this also is the name of the model or blueprint   |\n| Blender Model Types | Type of blender model, if applicable  |\n| Sample Type         | Type of training sample for the project; values may include Sample Percent, Row Count, Duration, etc.     |\n| Sample Length       | Amount of sample data for training the project; values are based on Sample Type and may be percentage, number of rows, or length of time      |\n| Model Fit Time      | Amount of time (in seconds) used to build the model    |\n| Recommended Model   | Identifies if this is the recommended model for deployment (true) or not (false)    |\n| Insight Type        | Type of insight requested for this model; possible values may include Variable Importance, Compute Series Accuracy, Compute Accuracy Over Time, Dual Lift, etc.    |\n| Custom Template     | Identifies if the compliance document (for the event) was developed with a custom template (true) or not (false); applies to Compliance Doc Generated events    |\n| Deployment ID       | ID for the deployment; applies to Replaced Model events   |\n| Deployment Type     | Type of deployment; applies to events such as Replaced Model and Deployment Added, and possible values include Dedicated Prediction (deployment to a dedicated prediction server) or Secure Worker (in-app modeling workers used for predictions) |\n| Client Type         | Client (DataRobotPythonClient or DataRobotRClient) used to interface with DataRobot; applies to events such as DataSet Upload, Project Created, Project Target Selected, Select Model Metric, etc.     |\n| Client Version      | Version of the related client (DataRobotPythonClient or DataRobotRClient)|\n| Catalog ID          | The ID of an item in the catalog. It can be used to address an individual item. Catalog ID is the same as a Dataset ID when the catalog item is a dataset.  |\n| Catalog Version ID  | An ID indicating a specific version of a catalog item. Catalog items can have multiple versions; by default the catalog uses the latest version. To work with an earlier version, you must use the specific version ID. |\n| Dataset ID          | ID of dataset |\n| Dataset Name (*)    | Name of dataset   |\n| Dataset Size        | Size of project dataset |\n| Snapshotted State   | A dataset can be  [snapshotted](glossary/index#snapshot) (materialized) or not. If it is snapshotted, the data has been stored locally. If it is not, the data is requested from the source whenever it is used. |\n| Grantee             | The UID of the user sharing the asset |\n| With Grant          | Indicates whether the user receiving the new role is allowed to share with others   |\n\n\n(\\*) denotes an [identifying field](#hide-sensitive-information) for this report\n\n## Prediction Usage activity report {: #prediction-usage-activity-report }\n\nThe Prediction Usage activity output reports the following data about prediction statistics.\n\n|  Report field  |     Description    |\n|----------------|--------------------|\n| Timestamp—UTC      | Timestamp (UTC time standard) when the prediction event occurred |\n| UID| ID for the user who initiated this event   |\n| Username (*)         | Name for the user who initiated this event     |\n| Project ID              | ID of the project      |\n| Org ID                  | ID of the user\'s organization   |\n| Org Name (*)            | Name of the user\'s organization    |\n| Group ID                | ID of the user\'s group     |\n| Group Name (*)          | Name of the user\'s group |\n| User Role               | Role for the user who initiated the prediction event; values include OWNER, USER, OBSERVER    |\n| Model ID                | ID of the model    |\n| Model Type              | Type for the model; this also is the name of the model or blueprint    |\n| Blender Model Types     | Type of blender model, if applicable    |\n| Recommended Model       | Identifies if this is the recommended model for deployment (true) or not (false)   |\n| Project Type            | Type for the project; possible values include Binary Classification, Regression, Time Series—Regression, Multiclass Classification, etc.     |\n| Deployment ID           | ID of the deployment   |\n| Deployment Type         | Type of deployment; possible values include dedicated (deployment to a dedicated prediction server) or Secure Worker (in-app modeling workers used for predictions)      |\n| Dataset ID              | ID of the dataset    |\n| Prediction Method       | Method for making predictions for the related project; possible values are Modeling Worker (predictions using modeling workers) or Dedicated Prediction (predictions using dedicated prediction server) |\n| Prediction Explanations | Identifies if Prediction Explanations were computed for this project model (true) or not (false)       |\n| # of Requests           | Number of prediction requests the deployment has received (where a single request can contain multiple prediction requests); provides statistics for deployment service health    |\n| # Rows Scored           | Number of dataset rows scored (for predictions) for this deployment    |\n| User Errors             | Number of user errors (4xx errors) for this deployment; provides statistics for deployment service health    |\n| Server Errors           | Number of server errors (5xx errors) for this deployment; provides statistics for deployment service health    |\n| Average Execution Time  | Average time (in milliseconds) DataRobot spent processing prediction requests for this deployment; provides statistics for deployment service health      |\n\n\n(\\*) denotes an [identifying field](#hide-sensitive-information) for this report\n\n## Self-Managed AI Platform admins {: #self-managed-ai-platform-admins }\n\nThe system information report is available only on the Self-Managed AI Platform.\n\n### System Information report {: #system-information-report }\n\nSystem information is available for download only, using [Export CSV](main-uam-overview#download-activity-data) (online preview is not available).\n\nThe System Information report provides information (key:value pairs) specific to the deployed cluster. The category of information is dependent on the type of deployed cluster.\n\n|  Report field  |     Description    |\n|----------------|--------------------|\n| deployment | Python version used to deploy the cluster. |\n| install type | The type of installation for the deployed cluster. |\n| mongo          |  Mongo database configuration. Identifies whether secrets are enforced when communicating with the Mongo database and also database availability.    |\n| redis    |  Redis queue service configuration. Identifies whether secrets are enforced when communicating with a Redis database and also database availability.  |\n| postgresql |   Postgres database configuration. Identifies whether secrets are enforced when communicating with the Postgres database.    |\n| elasticsearch   | Elasticsearch configuration. Identifies whether secrets are enforced  when communicating with Elasticsearch and lists configuration parameters and their availability.   |\n| tls config |  Identifies which services are set up to use transport layer security (TLS). |\n| modeling workers        | Reports the number of modeling workers configured. If a cluster has unlimited modeling workers, this field is empty.  |\n| dedicated predictions   | Identifies whether a dedicated prediction environment is configured and if audit logs are enabled on the environment.   |\n| smtp       |  Identifies whether SMTP integration is configured.   |\n| product type   | Lists DataRobot products (i.e. MLOps, AutoML, Time series) enabled for the deployed cluster.  |\n\n Typical information for a report is shown below:\n\n ![](images/sysinfo.png)\n\n', ""SHAP Prediction Explanations estimate how much each feature contributes to a given prediction differing from the average. They are intuitive, unbounded (computed for all features), fast, and, due to the open source nature of SHAP, transparent. Not only does SHAP provide the benefit of helping you better understand model behavior&mdash;and quickly&mdash;it also allows you to easily validate if a model adheres to business rules. \n\nUse SHAP to understand, for each model decision, which features are key. What drives a particular customer's decision to buy&mdash;age? gender? buying habits?&mdash;what is the magnitude on the decision for each factor?\n\n\n\n"", ""---\ntitle: Prepare custom models\ndescription: Prepare to create deployments from custom models\n\n---\n\n# Prepare custom models for deployment\n\nCustom inference models allow you to bring your own pre-trained models to DataRobot. By uploading a model artifact to the Custom Model Workshop, you can create, test, and deploy custom inference models to a centralized deployment hub. DataRobot supports models built with a variety of coding languages, including Python, R, and Java. If you've created a model outside of DataRobot and you want to upload your model to DataRobot, you need to define two components:\n\n* **Model content**: The compiled artifact, source code, and additional supporting files related to the model.\n\n* **Model environment**: The Docker image where the model will run. Model environments can be either _drop-in_ or _custom_, containing a Docker file and any necessary supporting files. DataRobot provides a variety of built-in environments. Custom environments are only required to accommodate very specialized models and use cases.\n\n!!! note\n    Custom inference models are _not_ custom DataRobot models. They are _user-defined_ models created outside of DataRobot and assembled in the Custom Model Workshop for deployment, monitoring, and governance.\n\nSee the associated [feature considerations](#feature-considerations) for additional information.\n\n## Custom Model Workshop\n\nTopic | Describes\n------|-----------\n[Custom Model Workshop](custom-model-workshop/index) | How you can bring your own pre-trained models into DataRobot as custom inference models and deploy these models to a centralized deployment hub.\n[Create custom models](custom-inf-model) | How to create custom inference models in the Custom Model Workshop.\n[Manage custom model dependencies](custom-model-dependencies) | How to manage model dependencies from the workshop and update the base drop-in environments to support your model code.\n[Manage custom model resource usage](custom-model-resource-mgmt) | How to configure the resources a model consumes to facilitate smooth deployment and minimize potential environment errors in production.\n[Add custom model versions](custom-model-versions) | How to to create a new version of the model and/or environment after updating the file contents with new package versions, different preprocessing steps, updated hyperparameters, and more.\n[Add training data to a custom model](custom-model-training-data) | How to add training data to a custom inference model for deployment.\n[Add files from a remote repo to a custom model](custom-model-repos) | How to connect to a remote repository and pull custom model files into the Custom Model Workshop.\n[Test a custom model in DataRobot](custom-model-test) | How to test custom inference models in the Custom Model Workshop.\n[Manage custom models](custom-model-actions) | How to delete or share custom models and custom model environments.\n[Register custom models](custom-model-reg) | How to register custom inference models in the Model Registry.\n\n## Custom model assembly\n\nTopic | Describes\n------|-----------\n[Custom model assembly](custom-model-assembly/index) | How to assemble the files required to run custom inference models.\n[Custom model components](custom-model-components) | How to identify the components required to run custom inference models.\n[Assemble structured custom models](structured-custom-models) | How to use DRUM to assemble and validate structured custom models compatible with DataRobot.\n[Assemble unstructured custom models](unstructured-custom-models) | How to use DRUM to assemble and validate unstructured custom models compatible with DataRobot.\n[DRUM CLI tool](custom-model-drum) | How to download and install DataRobot user model (DRUM) to work with Python, R, and Java custom models and to quickly test custom models, and custom environments locally before uploading into DataRobot.\n[Test a custom model locally](custom-local-test) | How to test custom inference models in your local environment using the DataRobot Model Runner (DRUM) tool.\n\n## Custom model environments\n\nTopic | Describes\n------|-----------\n[Custom model environments](custom-model-environments/index) | How to select a custom model environment from the drop-in environments or create additional custom environments.\n[Drop-in environments](drop-in-environments) | How to select the appropriate DataRobot drop-in environment when creating a custom model.\n[Custom environments](custom-environments) | How to assemble, validate, and upload a custom environment.\n\n##  Feature considerations {: #feature-considerations }\n\n* The creation of deployments using model images cannot be canceled while in progress.\n* Inference models receive raw CSV data and must handle all preprocessing themselves.\n* A model's existing training data can only be _changed_ if the model is not actively deployed. This restriction is not in place when adding training data for the first time. Also, training data cannot be unassigned; it can only be changed once assigned.\n* The target name can only be changed if a model has no training data and has not been deployed.\n* There is a per-user limit on the number of custom model deployments (30), custom environments (30), and custom environment versions (30) you can have.\n* Custom inference model server start-up is limited to 3 minutes.\n* The file size for training data is limited to 1.5GB.\n* Dependency management only works with packages in a proper index. Packages from URLs cannot be installed.\n* Unpinned python dependencies are not updated once the dependency image has been built. To update to a newer version, you will need to create a new requirements file with version constraints. DataRobot recommends always pinning versions.\n* *SaaS AI Platform only*: Custom inference models have no access to the internet and outside networks.\n"", '---\ntitle: Prediction Distribution graph\ndescription: The Prediction Distribution graph on the ROC Curve tab helps you evaluate classification models by showing the distribution of actual values in relation to the prediction threshold.\n\n---\n\n# Prediction Distribution graph {: #prediction-distribution-graph }\n\n\nThe Prediction Distribution graph (on the **[ROC Curve](roc-curve-tab-use)** tab) illustrates the distribution of actual values in relation to the [display threshold](threshold#set-the-display-threshold) (a dividing line for interpreting results).\n\nTo use the Prediction Distribution graph:\n\n1. Select a model on the Leaderboard and navigate to **Evaluate > ROC Curve**.\n\n2. Select a [data source](threshold#select-data-for-visualizations) and set the [display threshold](threshold#set-the-display-threshold). The Prediction Distribution graph updates, showing the display threshold line.\n\n    ![](images/roc-prediction-graph.png)\n\n    Every prediction to the left of the dividing line is classified as ""false"" and every prediction to the right of the dividing line is classified as ""true.""\n\n    The Prediction Distribution graph visually expresses model performance for the selected data source. Based on [Classification use case 2](roc-curve-tab-use#classification-use-case-2), this Prediction Distribution graph shows the predicted probabilities for the two groups of patients (readmitted and not readmitted), illustrating how well your model discriminates between them. The colors correspond to the rows of the confusion matrix&mdash;red represents patients not readmitted, blue represents readmitted patients. You can see that both red and blue fall on either side of the [display threshold](threshold#set-the-display-threshold).\n\n3. Interpret the graph using this table:\n\n    | Color on graph | Location | State |\n    |---|---|---|\n    | red | left of the threshold | true negative (TN) |\n    | blue | left of the threshold | false negative (FN) |\n    | red | right of the threshold | false positive (FP) |\n   | blue | right of the threshold | true positive (TP) |\n\n    Note that the gray represents the overlap of red and blue.\n\n    With a classification problem, each prediction corresponds to a single observation (readmitted or not, in this example). The Prediction Distribution graph shows the overall distribution of the predictions for all observations in the selected data source.\n\n4. Select one of the following from the **Y-Axis** dropdown. The **Y-Axis** distribution selector allows you to choose between showing the Prediction Distribution graph as a density or frequency curve:\n\n    === ""Density""\n\n        The chart displays an equal area underneath both the positive and negative curves.\n\n        ![](images/roc-prediction-graph-density.png)\n\n    === ""Frequency""\n\n        The area underneath each curve varies and is determined by the number of observations in each class.\n\n        ![](images/roc-prediction-graph-frequency.png)\n\n    The distribution curves are based on the data source and/or distribution selection. Alternating between **Frequency** and **Density** changes the curves but does not change the threshold or any values in the associated page elements.\n\n\n## Experiment with the Prediction Distribution graph {: #experiment-with-the-prediction-distribution-graph }\n\nTry the following changes and observe the results.\n\n1. Pass your cursor over the Prediction Distribution graph. The threshold value displays in white text as you move your cursor.\n\n    ![](images/roc-prediction-hover.png)\n\n    For curves displayed in the **Chart** pane (a ROC curve shown here), DataRobot displays a circle that dynamically moves to correspond with the threshold value.\n\n2. Click on the Prediction Distribution graph to select a new threshold value.\n\n    ![](images/roc-prediction-distribution-updated.png)\n\n    The new value appears in the **Display Threshold** field. The circle and intercept lines on the Prediction Distribution graph update to the new threshold value. The Metrics pane, the Chart pane (set to **ROC Curve** here), and the Matrix pane (set to **Confusion matrix** here) also update to reflect the new threshold.\n\n    Alternatively, you can [change the threshold setting](threshold#set-the-display-threshold) by typing a new value in the threshold field.\n\n3. Click the **Y-Axis** dropdown to switch the prediction\'s distribution between displaying a **Density** or **Frequency** curve. This change does not impact other page elements.\n']"
"[""---\ntitle: Cell actions\ndescription: Describes the various actions available to control notebook cells.\nsection_name: Notebooks\n---\n\n# Cell actions {: #cell-actions }\n\n{% include 'includes/notebooks/action-nb.md' %}\n"", '---\ntitle: Visual AI reference\ndescription: Do a deep dive on Visual AI in DataRobot.\n\n---\n\n# Visual AI reference {: #visual-ai-reference }\n\nThese sections describe the workflow and reference materials for including images as part of your DataRobot project.\n\nTopic | Describes...\n----- | ------\n[Visual AI reference](vai-ref) | Learn about technological components of Visual AI.\n[Visual AI tuning walkthrough](vai-tuning-guide) | See an example of the [tuning section](vai-tuning) at work. \n\nSee considerations for working with [Visual AI](vai-model#feature-considerations).\n']"
"['---\ntitle: Modeling FAQ\ndataset_name: N/A\nDescription: Provides a list of frequently asked questions, and brief answers about general modeling, building models, and model insights in DataRobot. Answers link to more complete documentation.\ndomain: core modeling\nexpiration_date: 10-10-2024\nowner: jen@datarobot.com\nurl: docs.datarobot.com/docs/tutorials/create-ai-models/general-modeling-faq.html\n---\n\n# Modeling FAQ {: #modeling-faq }\n\nThe following addresses questions and answers about modeling in general and then more specifically about building models and using model insights.\n\n## General modeling {: #general-modeling }\n\n??? faq ""What types of models does DataRobot build?""\n\tDataRobot supports Tree-based models, Deep Learning models, Support Vector Machines (SVM), Generalized Linear Models, Anomaly Detection models, Text Mining models, and more. See the list of [specific model types](model-ref#modeling-modes) for more information.\n\n??? faq ""What are modeling workers?""\n\tDataRobot uses different types of workers for different types of jobs; modeling workers are for training models and creating insights. You can adjust these workers in the [Worker Queue](worker-queue), which can speed model building and allow you to allocate across projects.\n\n??? faq ""Why can\'t I add more workers?""\n\tYou may have reached your maximum, if in a shared pool your coworkers may be using them, or they may be in use with another project. See the [troubleshooting tips](worker-queue#troubleshoot-workers) for more information.\n\n??? faq ""What is the difference between a model and a blueprint?""\n\tA *modeling algorithm* fits a model to data, which is just one component of a blueprint. A *blueprint* represents the high-level end-to-end procedure for fitting the model, including any preprocessing steps, modeling, and post-processing steps. Read about accessing the [graphical representation of a blueprint](blueprints).\n\n??? faq ""What is smart downsampling?""\n\t[Smart downsampling](smart-ds) is a technique to reduce the total size of the dataset by reducing the size of the majority class, enabling you to build models faster without sacrificing accuracy.\n\n??? faq ""What are EDA1 and EDA2?""\n\t[Exploratory data analysis](eda-explained), or EDA, is DataRobot\'s approach to analyzing datasets and summarizing their main characteristics. It consists of two phases. EDA1 describes the state of your project after data finishes uploading, providing summary statistics based on up to 500MB of your data. In EDA2, DataRobot does additional calculations on the target column using the entire dataset (excluding holdout) and  recalculates summary statistics and ACE scores.\n\n??? faq ""What does a Leaderboard asterisk mean?""\n\tAn [asterisk on the Leaderboard](leaderboard-ref#asterisked-scores) indicates that the scores are computed from [stacked predictions](data-partitioning#what-are-stacked-predictions) on the model\'s training data.\n\n??? faq ""What does the Leaderboard snowflake icon mean?""\n\tThe snowflake next to a model indicates that the model is the result of a [frozen run](frozen-run). In other words, DataRobot “froze” parameter settings from a model’s early, small sample size-based run. Because parameter settings based on smaller samples tend to also perform well on larger samples of the same data, DataRobot can piggyback on its early experimentation.\n\n??? faq ""What is cross-validation?""\n\tCross validation is a [partitioning method](data-partitioning#k-fold-cross-validation-cv) for evaluating model performance. It is run automatically for datasets less than 50,000 rows and can be started manually from the Leaderboard for larger datasets.\n\n??? faq ""What do the modes and sample sizes mean?""\n\tThere are several [modeling mode options](model-data#set-the-modeling-mode); the selected mode determines the sample size(s) of the run. Autopilot is DataRobot\'s ""survival of the fittest"" modeling mode that automatically selects the best predictive models for the specified target feature and runs them at ever-increasing sample sizes.\n\n??? faq ""Why are the sample sizes shown in the repository not the standard Autopilot sizes?""\n\tThe sample size available when adding models from the [Repository](repository) differs depending on the size of the dataset. It defaults to the last Autopilot stage, either 64% or 500MB of data, whichever is smaller. In other words, it is the maximal [training size](repository#notes-on-sample-size) without stepping into validation.\n\n\n??? faq ""Are there modeling guardrails?""\n\tDataRobot provides guardrails to help ensure ML best practices and instill confidence in DataRobot models. Some examples include a substantive data [quality assessment](data-quality), a feature list with [target leakage features removed](feature-lists#automatically-created-feature-lists), and automated [data drift tracking](data-drift).\n\n??? faq ""How are missing values handled?""\n\tDataRobot handles missing values differently, depending on the model and/or value type. There are [certain patterns](model-ref#missing-values) recognized and handled as missing, as well as [disguised missing value](quality-check#disguised-missing-values) handling.\n\n## Build models {: #build-models }\n\n??? faq ""Does DataRobot support feature transformations?""\n\tIn AutoML, DataRobot performs [automatic feature transformations](auto-transform) for features recognized as type “date,” adding these new features to the modeling dataset. Additionally, you can create [manual transformations](feature-transforms) and change variable type. For image datasets, the [train-time image augmentation](ttia-lists) process creates new training images. The [time series feature derivation](ts-create-data#create-the-modeling-dataset) process creates a new modeling dataset. [Feature Discovery](fd-overview) discovers and generates new features from multiple datasets to consolidate datasets. Or, use a [Spark SQL query](spark) from the AI Catalog to prepare a new dataset from a single dataset or blend two or more datasets. Transformed features are marked with an info icon on the data page.\n\n??? faq ""Can I choose which optimization metric to use?""\n    The optimization metric defines how to score models. DataRobot selects a metric best-suited for your data from a [comprehensive set of choices](opt-metric), but also computes alternative metrics. After [EDA1](eda-explained#eda1) completes, you can [change the selection](additional#change-the-optimization-metric) from the **Advanced Options > Additional** tab. After [EDA2](eda-explained#eda2) completes, you can redisplay the Leaderboard listing based on a different computed metric.\n\n??? faq ""Can I change the project type?""\n    Once you enter a target feature, DataRobot automatically analyzes the training dataset, determines the project type (classification if the target has categories or regression if the target is numerical), and displays the distribution of the target feature. If the project is classified as regression and [eligible for multiclass conversion](multiclass#change-regression-projects-to-multiclass), you can change the project to a classification project, and DataRobot will interpret values as classes instead of continuous values.\n\n??? faq ""How do I control how to group or partition my data for model training?""\n\tBy default, DataRobot splits your data into a 20% holdout (test) partition and an 80% cross-validation (training and validation) partition, which is divided into five sub-partitions. You can change these values after loading data and selecting a target from the **Advanced Options > Partitioning** tab. From there, you can [set the method](partitioning), sizes for data partitions, number of partitions for cross-validation, and the method by which those partitions are created.\n\n??? faq ""What do the green ""importance"" bars represent on the Data tab?""\n\tThe [Importance green bars](model-ref#importance-score), based on [""Alternating Conditional Expectations""](https://www.jds-online.com/files/JDS-156.pdf) (ACE) scores,  show the degree to which a feature is correlated with the target. Importance has two components&mdash;Value and Normalized Value&mdash;and is calculated independently for each feature in the dataset.\n\n??? faq ""Does DataRobot handle natural language processing (NLP)?""\n\tWhen text fields are detected in your data, DataRobot automatically detects the language and applies appropriate preprocessing. This may include advanced tokenization, data cleaning (stop word removal, stemming, etc.), and vectorization methods. DataRobot supports n-gram matrix (bag-of-words, bag-of-characters) analysis as well as word embedding techniques such as Word2Vec and fastText with both CBOW and Skip-Gram learning methods. Additional capabilities include Naive Bayes SVM and cosine similarity analysis. For visualization, there are per-class word clouds for text analysis. You can see the applied language preprocessing steps in the [model blueprint](blueprints).\n\n\n??? faq ""How do I restart a project with the same data?""\n\tIf your data is stored in the AI Catalog, you can [create and recreate](catalog#create-a-project) projects from that dataset. To recreate a project&mdash;using either just the data or the data and the settings (i.e., to duplicate the project)&mdash;use the [**Actions** menu](manage-projects#duplicate-a-project) in the project control center.\n\n\n??? faq ""Do I have to use the UI or can I interact programmatically?""\n\tDataRobot provides both a UI and a REST API. The UI and REST API provide nearly matching functionality. Additionally, [Python and R clients](https://docs.datarobot.com/en/api/) provide a subset of what you can do with the full API.\n\n??? faq ""Does DataRobot provide partner integrations?""\n\tDataRobot offers an an [Alteryx](alteryx) add-in and a [Tableau](tableau) extension. A [Snowflake integration](fd-overview#snowflake-integration) allows joint users to execute Feature Discovery projects in DataRobot while performing computations in Snowflake for minimized data movement.\n\n=== ""SaaS""\n\t??? faq\t""What is the difference between prediction and modeling servers?""\n\t\tModeling servers power all the creation and model analysis done from the UI and from the R and Python clients. Prediction servers are used solely for making predictions and handling prediction requests on deployed models.\n\n=== ""Self-Managed""\n\t??? faq\t""What is the difference between prediction and modeling servers?""\n\t\tModeling servers power all the creation and model analysis done from the UI and from the R and Python clients. Modeling worker resources are reported in the [Resource Monitor](resource-monitor). Prediction servers are used solely for making predictions and handling prediction requests on deployed models.\n\n## Model insights {: #model-insights}\n\n??? faq ""How do I directly compare model performance?""\n\tThere are many ways to compare model performance. Some starter points:\n\n\t* Look at the Leaderboard to compare [Validation, Cross-Validation, and/or Holdout scores](leaderboard-ref#columns-and-tools).\n\t* Use [Learning Curves](learn-curve) to help determine whether it is worthwhile to increase the size of your dataset for a given model. The results help identify which models may benefit from being trained into the Validation or Holdout partition.\n\t* [Speed vs Accuracy ](speed) compares multiple models in a measure of the tradeoff between runtime and predictive accuracy. If prediction latency is important for model deployment then this will help you find the most effective model.\n\t* [Model Comparison](model-compare) lets you select a pair of models and compare a variety of insights (Lift Charts, Profit Curve, ROC Curves).\n\n??? faq ""How does DataRobot choose the recommended model?""\n\tAs part of the Autopilot modeling process, DataRobot identifies the most accurate non-blender model and [prepares it for deployment](model-rec-process). Although Autopilot recommends and prepares a single model for deployment, you can initiate the Autopilot recommendation and deployment preparation stages for any Leaderboard model.\n\n??? faq ""Why not always use the most accurate model?""\n\tThere could be several reasons, but the two most common are:\n\n\t* Prediction latency&mdash;This means the speed at which predictions are made. Some business applications of a model will require very fast predictions on new data. The most accurate models are often blender models which are usually slower at making predictions.\n\t* Organizational readiness&mdash;Some organizations favor linear models and/or decision trees for perceived interpretability reasons. Additionally, there may be compliance reasons for favoring certain types of models over others.\n\n??? faq ""Why doesn’t the recommended model have text insights?""\n\tOne common reason that text models are not built is because DataRobot removes single-character ""words"" when model building, a common practice in text mining. If this causes a problem, look at your [model log](log) and consider the [documented workarounds](analyze-insights#text-based-insights).\n\n??? faq ""What is model lift?""\n\tLift is the ratio of points correctly classified as positive in a model versus the 45-degree line (or baseline model) represented in the [Cumulative Gain](cumulative-charts#cumulative-gain-chart) plot. The cumulative charts show, for a given % of top predictions, how much more effective the selected model is at identifying the positive class versus the baseline model.\n\n??? faq ""What is the ROC Curve chart?""\n\tThe [ROC Curve](roc-curve-tab/index) tab provides extensive tools for exploring classification, performance, and statistics related to a selected model at any point on the probability scale. Documentation discusses prediction thresholds, the Matthews Correlation Coefficient (MCC), as well as interpreting the ROC Curve, Cumulative Gain, and Profit Curve charts.\n\n??? faq ""Can I tune model hyperparameters?""\n\tYou can tune model hyperparameters in the [Advanced Tuning](adv-tuning) tab for a particular model. From here, you can manually set model parameters, overriding the DataRobot selections. However, consider whether it is instead better to spend “tuning” time doing feature engineering, for example using [Feature Discovery](fd-overview) for  automated feature engineering.\n\n??? faq ""How is Tree-Based Variable Importance different from Feature Impact?""\n\t[Feature Impact](feature-impact) shows, at a high level, which features are driving model decisions. It is computed by permuting the rows of a given feature while leaving the rows of the other features unchanged, and measuring the impact of the permutation on the model\'s predictive performance. [Tree-based Variable Importance](analyze-insights#tree-based-variable-importance) shows how much gain each feature adds to a model--the relative importance of the key features. It is only available for tree/forest models (for example, Gradient Boosted Trees Classifier or Random Forest).\n\n??? faq ""How can I find models that produced coefficients?""\n\tAny model that produces coefficients can be identified on the Leaderboard with a Beta (![](images/icon-cf-export.png)) tag. Those models allow you to [export the coefficients](coefficients#generate-output) and transformation parameters necessary to verify steps and make predictions outside of DataRobot. When a blueprint has coefficients but is not marked with the Beta tag, it indicates that the coefficients are not exact (e.g., they may be rounded).\n\n??? faq ""What is the difference between ""text mining"" and ""word clouds""?""\n\tThe [Text Mining](analyze-insights#text-mining-insights) and [Word Cloud](analyze-insights#word-cloud-insights) insights demonstrate text importance in different formats. *Text Mining* shows text coefficient effect (numeric value) and direction (positive=red or negative=blue) in a bar graph format. The Word Cloud shows the normalized version of those coefficients in a cloud format using text size and color.\n\n??? faq ""Why are there variables in some insights that are not in the dataset?""\n\tDataRobot performs a variety of data preprocessing, such as [automatic transformations](auto-transform) and deriving features (for example, ratios and differences). When building models, it uses all useful features, which includes both original and derived variables.\n\n??? faq ""Why does Feature Effects show missing partial dependence values when my dataset has none?""\n\tPartial dependence (PD) is reported as part of Feature Effects. It shows how dependent the prediction value is on different values of the selected feature. Prediction values are affected by all features, though, not just the selected feature, so PD must measure how predictions change given different values of the other features as well. When computing, DataRobot adds “missing” as one of the values [calculated](feature-effects#partial-dependence-calculations) for the selected feature, to show how the absence of a value will affect the prediction. The end result is the average effect of each value on the prediction, given other values, and following the distribution of the training data.\n\n??? faq ""How do I determine how long it will take to calculate Feature Effects?""\n\tIt can take a long time to compute Feature Effects, particularly if blenders are involved. As a rough estimate of the runtime, use the [Model Info](model-info) tab to check the time it takes, in seconds, for your model to score 1000 rows. Multiply this number by 0.5-1.0 hours. Note that the actual runtime may be longer if you don’t assign enough workers to work on all Feature Effects sub-jobs simultaneously.\n\n??? faq ""Why is a feature’s impact different depending on the model?""\n\tAutopilot builds a wide selection of models to capture varying degrees of underlying complexity and each model has its strengths and weaknesses in addressing that complexity. [A feature’s impact](feature-impact) shouldn\'t be drastically different, however, so while the ordering of features will change, the overall inference is often not impacted. Examples:\n\n\t* A model that is not capable of detecting nonlinear relationships or interactions will use the variables one way, while a model that can detect these relationships will use the variables another way. The result is different feature impacts from different models.\n\t* If two variables are highly correlated, a regularized linear model will tend to use only one of them, while a tree-based method will tend to use both, and at different splits. With the linear model, one of these variables will show up high in feature importance and the other will be low, while with the tree-based model, both will be closer to the middle.\n', '# Version 7.1.2 {: #version-712 }\n\n_August 2, 2021_\n\nThe DataRobot v7.1.2 release includes some fixed issues in the DataRobot Self-Managed AI Platform platform. See the v7.1.0 release notes for:\n\n* [Features introduced in v7.1.0](v7.1.0-aml)\n\n## Updated language localization {: #updated-language-localization }\n\nLocalization of the documentation has been updated to include Japanese content for all new 7.1 features.\n\nTo change languages, open your [user settings](getting-help) and select a language for your session. The selection remains until you reset it.\n\n![](images/languages-set-pt-jp.png)\n\n## Issues fixed in v7.1.2 {: #issues-fixed-in-v712 }\n\nThe following issues have been fixed since Enterprise release v7.1.1:\n\n### Enterprise {: #enterprise }\n\n* EP-1226: Fixes an issue where `patroni` installs failed because nodes were not syncing.\n* EP-1345: Fixes an issue that caused a KeyError during DB migration.\n\n### Platform {: #platform }\n\n* PRODSEC-234: Prevents containers from acquiring new privileges.\n* XAI-4238: Fixes an issue when loading certain models with SHAP explainers that were built in DataRobot version 5.2.2, which failed to load after upgrading to version 6.3.\n\n### Feature Discovery {: #feature-discovery }\n\n* SAFER-3964: Fixes an issue where Feature Discovery projects created before DataRobot version 7.1 resulted in an error when running predictions.\n\n### Time Series {: #time-series }\n\n* TIME-8745: Allows `COMPUTE_TIMESERIES_AOT_THRESHOLD` to be configured from the environment.\n\n_All product and company names are trademarks&trade; or registered&reg; trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them_.\n', '---\ntitle: Use feature engineering and Visual AI with acoustic data\ndescription: Generate image features in addition to aggregate numeric features for high frequency data sources.\n\n---\n\n# Use feature engineering and Visual AI with acoustic data {: #use-feature-engineering-and-visual-ai-with-acoustic-data }\n\n[Access this AI accelerator on GitHub <span style=""vertical-align: sub"">:material-arrow-right-circle:{.lg }</span>](https://github.com/datarobot-community/ai-accelerators/blob/main/use_cases_and_horizontal_approaches/high_frequency_data_classification_using_spectrograms_n_numerics/high_frequency_classification_spectrograms_n_numerics.ipynb){ .md-button }\n\nThe density of high frequency data presents a challenge for standard machine learning workflows that lack specialized feature engineering techniques to condense the signal, extracting and highlighting its uniqueness. DataRobot\'s multimodal input capability supports simultaneously leveraging numerics and images, which for this use-case is particularly beneficial for including descriptive spectrograms that enable you to leverage well-established computer vision techniques for complex data understanding.\n\nThis example notebook shows how to generate image features and aggregate numeric features for high frequency data sources. This approach converts audio wav files from the time domain into the frequency domain to create several types of spectrograms. Statistical numeric features computed from the converted signal add additional descriptors to aid classification of the audio source.\n']"
"[""---\ntitle: Evaluate\ndescription: The Evaluate tabs provide key plots and statistics needed to judge a model's effectiveness, including ROC Curve, Lift Chart, and Forecasting Accuracy.\n\n---\n\n# Evaluate {: #evaluate }\n\nThe **Evaluate** tabs provide key plots and statistics needed to judge and interpret a model’s effectiveness:\n\nLeaderboard tab  | Description | Source\n------------------|-------------|------------\n[Accuracy Over Space](lai-insights) | Provides a spatial residual mapping within an individual model. |  Validation, Cross-Validation, Holdout (selectable)\n[Accuracy over Time](aot)  | Visualizes how predictions change over time. | Computed separately for each backtest and the Holdout fold and can be viewed in the UI. Plots can be computed on both Validation and Training data.\n[Advanced Tuning](adv-tuning) | Visualizes how predictions change over time. | Internal grid search set\n[Anomaly Assessment](anom-viz)  | Plots data for the selected backtest and provides SHAP explanations for up to 500 anomalous points. | Computed separately for each backtest and the Holdout fold and can be viewed in the UI. Plots can be computed on both Validation and Training data.\n[Anomaly over Time](anom-viz) | Plots how anomalies occur across the timeline of your data. | Computed separately for each backtest and the Holdout fold and can be viewed in the UI. Plots can be computed on both Validation and Training data.\n[Confusion Matrix for multiclass projects](multiclass)  | Compares actual data values with predicted data values in multiclass projects. | Validation, Cross-Validation, or Holdout (selectable). For binary classification projects, use the [confusion matrix](confusion-matrix) on the [ROC Curve](roc-curve-tab/index) tab.\nFeature Fit | Removed. See [**Feature Effects**](feature-effects).\n[Forecasting Accuracy](forecast-acc)  | Provides a visual indicator of how well a model predicts at each forecast distance in the project’s forecast window. | Computed separately for each backtest and the Holdout fold; only the validation subset of each fold is scored. Validation predictions are filtered by the forecast distance and the metrics are computed on the filtered predictions. UI/API does not provide access to individual backtests but rather to validation (backtest 0=most recent backtest), backtesting (averaged across all backtests), and Holdout.\n[Forecast vs Actual](fore-act) | Compares how different predictions behave at different forecast points to different times in the future. | Computed separately for each backtest and the Holdout fold and can be viewed in the UI. Plots can be computed on both Validation and training data.\n[Lift Chart](lift-chart) | Depicts how well a model segments the target population and how capable it is of predicting the target.  | Validation, Cross-Validation, Holdout (selectable)\n[Period Accuracy](ts-period-accuracy) |\tView model performance over periods within the training dataset. | Validation, Holdout (selectable). Computed separately for each backtest and Holdout. \n[Residuals](residuals)  | Clearly visualizes the predictive performance and validity of a regression model.  | Validation, Cross-Validation, Holdout (selectable)\n[ROC Curve](roc-curve-tab/index)  | Explores classification, performance, and statistics related to a selected model at any point on the probability scale. | Validation data\n[Series Insights (clustering)](series-insights)  |  Provides information on the cluster to which each series belongs, along with series information, including rows and dates. Histograms for each cluster show the number of series, the number of total rows, and the percentage of the dataset that belongs to that cluster. | Computed for each series in the clustering backtest.\n[Series Insights (multiseries)](series-insights-multi)  |  Provides series-specific information. | Computed separately for each backtest and the Holdout fold; only the validation subset of each fold is scored. Validation predictions are filtered by the forecast distance and the metrics are computed on the filtered predictions. UI/API does not provide access to individual backtests but rather to validation (backtest 0=most recent backtest), backtesting (averaged across all backtests), and Holdout.\n[Stability](stability) | Provides an at-a-glance summary of how well a model performs on different backtests. | Computed separately for each backtest and the Holdout fold; only the validation subset of each fold is scored.\n[Training Dashboard](training-dash) | Provides an understanding about training activity, per iteration, for Keras-based models. | Training, but validated on an internal holdout of the training data.\n""]"
"[""---\ntitle: Prepare custom models\ndescription: Prepare to create deployments from custom models\n\n---\n\n# Prepare custom models for deployment\n\nCustom inference models allow you to bring your own pre-trained models to DataRobot. By uploading a model artifact to the Custom Model Workshop, you can create, test, and deploy custom inference models to a centralized deployment hub. DataRobot supports models built with a variety of coding languages, including Python, R, and Java. If you've created a model outside of DataRobot and you want to upload your model to DataRobot, you need to define two components:\n\n* **Model content**: The compiled artifact, source code, and additional supporting files related to the model.\n\n* **Model environment**: The Docker image where the model will run. Model environments can be either _drop-in_ or _custom_, containing a Docker file and any necessary supporting files. DataRobot provides a variety of built-in environments. Custom environments are only required to accommodate very specialized models and use cases.\n\n!!! note\n    Custom inference models are _not_ custom DataRobot models. They are _user-defined_ models created outside of DataRobot and assembled in the Custom Model Workshop for deployment, monitoring, and governance.\n\nSee the associated [feature considerations](#feature-considerations) for additional information.\n\n## Custom Model Workshop\n\nTopic | Describes\n------|-----------\n[Custom Model Workshop](custom-model-workshop/index) | How you can bring your own pre-trained models into DataRobot as custom inference models and deploy these models to a centralized deployment hub.\n[Create custom models](custom-inf-model) | How to create custom inference models in the Custom Model Workshop.\n[Manage custom model dependencies](custom-model-dependencies) | How to manage model dependencies from the workshop and update the base drop-in environments to support your model code.\n[Manage custom model resource usage](custom-model-resource-mgmt) | How to configure the resources a model consumes to facilitate smooth deployment and minimize potential environment errors in production.\n[Add custom model versions](custom-model-versions) | How to to create a new version of the model and/or environment after updating the file contents with new package versions, different preprocessing steps, updated hyperparameters, and more.\n[Add training data to a custom model](custom-model-training-data) | How to add training data to a custom inference model for deployment.\n[Add files from a remote repo to a custom model](custom-model-repos) | How to connect to a remote repository and pull custom model files into the Custom Model Workshop.\n[Test a custom model in DataRobot](custom-model-test) | How to test custom inference models in the Custom Model Workshop.\n[Manage custom models](custom-model-actions) | How to delete or share custom models and custom model environments.\n[Register custom models](custom-model-reg) | How to register custom inference models in the Model Registry.\n\n## Custom model assembly\n\nTopic | Describes\n------|-----------\n[Custom model assembly](custom-model-assembly/index) | How to assemble the files required to run custom inference models.\n[Custom model components](custom-model-components) | How to identify the components required to run custom inference models.\n[Assemble structured custom models](structured-custom-models) | How to use DRUM to assemble and validate structured custom models compatible with DataRobot.\n[Assemble unstructured custom models](unstructured-custom-models) | How to use DRUM to assemble and validate unstructured custom models compatible with DataRobot.\n[DRUM CLI tool](custom-model-drum) | How to download and install DataRobot user model (DRUM) to work with Python, R, and Java custom models and to quickly test custom models, and custom environments locally before uploading into DataRobot.\n[Test a custom model locally](custom-local-test) | How to test custom inference models in your local environment using the DataRobot Model Runner (DRUM) tool.\n\n## Custom model environments\n\nTopic | Describes\n------|-----------\n[Custom model environments](custom-model-environments/index) | How to select a custom model environment from the drop-in environments or create additional custom environments.\n[Drop-in environments](drop-in-environments) | How to select the appropriate DataRobot drop-in environment when creating a custom model.\n[Custom environments](custom-environments) | How to assemble, validate, and upload a custom environment.\n\n##  Feature considerations {: #feature-considerations }\n\n* The creation of deployments using model images cannot be canceled while in progress.\n* Inference models receive raw CSV data and must handle all preprocessing themselves.\n* A model's existing training data can only be _changed_ if the model is not actively deployed. This restriction is not in place when adding training data for the first time. Also, training data cannot be unassigned; it can only be changed once assigned.\n* The target name can only be changed if a model has no training data and has not been deployed.\n* There is a per-user limit on the number of custom model deployments (30), custom environments (30), and custom environment versions (30) you can have.\n* Custom inference model server start-up is limited to 3 minutes.\n* The file size for training data is limited to 1.5GB.\n* Dependency management only works with packages in a proper index. Packages from URLs cannot be installed.\n* Unpinned python dependencies are not updated once the dependency image has been built. To update to a newer version, you will need to create a new requirements file with version constraints. DataRobot recommends always pinning versions.\n* *SaaS AI Platform only*: Custom inference models have no access to the internet and outside networks.\n"", '---\ntitle: Prediction Distribution graph\ndescription: The Prediction Distribution graph on the ROC Curve tab helps you evaluate classification models by showing the distribution of actual values in relation to the prediction threshold.\n\n---\n\n# Prediction Distribution graph {: #prediction-distribution-graph }\n\n\nThe Prediction Distribution graph (on the **[ROC Curve](roc-curve-tab-use)** tab) illustrates the distribution of actual values in relation to the [display threshold](threshold#set-the-display-threshold) (a dividing line for interpreting results).\n\nTo use the Prediction Distribution graph:\n\n1. Select a model on the Leaderboard and navigate to **Evaluate > ROC Curve**.\n\n2. Select a [data source](threshold#select-data-for-visualizations) and set the [display threshold](threshold#set-the-display-threshold). The Prediction Distribution graph updates, showing the display threshold line.\n\n    ![](images/roc-prediction-graph.png)\n\n    Every prediction to the left of the dividing line is classified as ""false"" and every prediction to the right of the dividing line is classified as ""true.""\n\n    The Prediction Distribution graph visually expresses model performance for the selected data source. Based on [Classification use case 2](roc-curve-tab-use#classification-use-case-2), this Prediction Distribution graph shows the predicted probabilities for the two groups of patients (readmitted and not readmitted), illustrating how well your model discriminates between them. The colors correspond to the rows of the confusion matrix&mdash;red represents patients not readmitted, blue represents readmitted patients. You can see that both red and blue fall on either side of the [display threshold](threshold#set-the-display-threshold).\n\n3. Interpret the graph using this table:\n\n    | Color on graph | Location | State |\n    |---|---|---|\n    | red | left of the threshold | true negative (TN) |\n    | blue | left of the threshold | false negative (FN) |\n    | red | right of the threshold | false positive (FP) |\n   | blue | right of the threshold | true positive (TP) |\n\n    Note that the gray represents the overlap of red and blue.\n\n    With a classification problem, each prediction corresponds to a single observation (readmitted or not, in this example). The Prediction Distribution graph shows the overall distribution of the predictions for all observations in the selected data source.\n\n4. Select one of the following from the **Y-Axis** dropdown. The **Y-Axis** distribution selector allows you to choose between showing the Prediction Distribution graph as a density or frequency curve:\n\n    === ""Density""\n\n        The chart displays an equal area underneath both the positive and negative curves.\n\n        ![](images/roc-prediction-graph-density.png)\n\n    === ""Frequency""\n\n        The area underneath each curve varies and is determined by the number of observations in each class.\n\n        ![](images/roc-prediction-graph-frequency.png)\n\n    The distribution curves are based on the data source and/or distribution selection. Alternating between **Frequency** and **Density** changes the curves but does not change the threshold or any values in the associated page elements.\n\n\n## Experiment with the Prediction Distribution graph {: #experiment-with-the-prediction-distribution-graph }\n\nTry the following changes and observe the results.\n\n1. Pass your cursor over the Prediction Distribution graph. The threshold value displays in white text as you move your cursor.\n\n    ![](images/roc-prediction-hover.png)\n\n    For curves displayed in the **Chart** pane (a ROC curve shown here), DataRobot displays a circle that dynamically moves to correspond with the threshold value.\n\n2. Click on the Prediction Distribution graph to select a new threshold value.\n\n    ![](images/roc-prediction-distribution-updated.png)\n\n    The new value appears in the **Display Threshold** field. The circle and intercept lines on the Prediction Distribution graph update to the new threshold value. The Metrics pane, the Chart pane (set to **ROC Curve** here), and the Matrix pane (set to **Confusion matrix** here) also update to reflect the new threshold.\n\n    Alternatively, you can [change the threshold setting](threshold#set-the-display-threshold) by typing a new value in the threshold field.\n\n3. Click the **Y-Axis** dropdown to switch the prediction\'s distribution between displaying a **Density** or **Frequency** curve. This change does not impact other page elements.\n', ""---\ntitle: Cell actions\ndescription: Describes the various actions available to control notebook cells.\nsection_name: Notebooks\n---\n\n# Cell actions {: #cell-actions }\n\n{% include 'includes/notebooks/action-nb.md' %}\n"", '---\ntitle: Visual AI reference\ndescription: Do a deep dive on Visual AI in DataRobot.\n\n---\n\n# Visual AI reference {: #visual-ai-reference }\n\nThese sections describe the workflow and reference materials for including images as part of your DataRobot project.\n\nTopic | Describes...\n----- | ------\n[Visual AI reference](vai-ref) | Learn about technological components of Visual AI.\n[Visual AI tuning walkthrough](vai-tuning-guide) | See an example of the [tuning section](vai-tuning) at work. \n\nSee considerations for working with [Visual AI](vai-model#feature-considerations).\n', '---\ntitle: Modeling FAQ\ndataset_name: N/A\nDescription: Provides a list of frequently asked questions, and brief answers about general modeling, building models, and model insights in DataRobot. Answers link to more complete documentation.\ndomain: core modeling\nexpiration_date: 10-10-2024\nowner: jen@datarobot.com\nurl: docs.datarobot.com/docs/tutorials/create-ai-models/general-modeling-faq.html\n---\n\n# Modeling FAQ {: #modeling-faq }\n\nThe following addresses questions and answers about modeling in general and then more specifically about building models and using model insights.\n\n## General modeling {: #general-modeling }\n\n??? faq ""What types of models does DataRobot build?""\n\tDataRobot supports Tree-based models, Deep Learning models, Support Vector Machines (SVM), Generalized Linear Models, Anomaly Detection models, Text Mining models, and more. See the list of [specific model types](model-ref#modeling-modes) for more information.\n\n??? faq ""What are modeling workers?""\n\tDataRobot uses different types of workers for different types of jobs; modeling workers are for training models and creating insights. You can adjust these workers in the [Worker Queue](worker-queue), which can speed model building and allow you to allocate across projects.\n\n??? faq ""Why can\'t I add more workers?""\n\tYou may have reached your maximum, if in a shared pool your coworkers may be using them, or they may be in use with another project. See the [troubleshooting tips](worker-queue#troubleshoot-workers) for more information.\n\n??? faq ""What is the difference between a model and a blueprint?""\n\tA *modeling algorithm* fits a model to data, which is just one component of a blueprint. A *blueprint* represents the high-level end-to-end procedure for fitting the model, including any preprocessing steps, modeling, and post-processing steps. Read about accessing the [graphical representation of a blueprint](blueprints).\n\n??? faq ""What is smart downsampling?""\n\t[Smart downsampling](smart-ds) is a technique to reduce the total size of the dataset by reducing the size of the majority class, enabling you to build models faster without sacrificing accuracy.\n\n??? faq ""What are EDA1 and EDA2?""\n\t[Exploratory data analysis](eda-explained), or EDA, is DataRobot\'s approach to analyzing datasets and summarizing their main characteristics. It consists of two phases. EDA1 describes the state of your project after data finishes uploading, providing summary statistics based on up to 500MB of your data. In EDA2, DataRobot does additional calculations on the target column using the entire dataset (excluding holdout) and  recalculates summary statistics and ACE scores.\n\n??? faq ""What does a Leaderboard asterisk mean?""\n\tAn [asterisk on the Leaderboard](leaderboard-ref#asterisked-scores) indicates that the scores are computed from [stacked predictions](data-partitioning#what-are-stacked-predictions) on the model\'s training data.\n\n??? faq ""What does the Leaderboard snowflake icon mean?""\n\tThe snowflake next to a model indicates that the model is the result of a [frozen run](frozen-run). In other words, DataRobot “froze” parameter settings from a model’s early, small sample size-based run. Because parameter settings based on smaller samples tend to also perform well on larger samples of the same data, DataRobot can piggyback on its early experimentation.\n\n??? faq ""What is cross-validation?""\n\tCross validation is a [partitioning method](data-partitioning#k-fold-cross-validation-cv) for evaluating model performance. It is run automatically for datasets less than 50,000 rows and can be started manually from the Leaderboard for larger datasets.\n\n??? faq ""What do the modes and sample sizes mean?""\n\tThere are several [modeling mode options](model-data#set-the-modeling-mode); the selected mode determines the sample size(s) of the run. Autopilot is DataRobot\'s ""survival of the fittest"" modeling mode that automatically selects the best predictive models for the specified target feature and runs them at ever-increasing sample sizes.\n\n??? faq ""Why are the sample sizes shown in the repository not the standard Autopilot sizes?""\n\tThe sample size available when adding models from the [Repository](repository) differs depending on the size of the dataset. It defaults to the last Autopilot stage, either 64% or 500MB of data, whichever is smaller. In other words, it is the maximal [training size](repository#notes-on-sample-size) without stepping into validation.\n\n\n??? faq ""Are there modeling guardrails?""\n\tDataRobot provides guardrails to help ensure ML best practices and instill confidence in DataRobot models. Some examples include a substantive data [quality assessment](data-quality), a feature list with [target leakage features removed](feature-lists#automatically-created-feature-lists), and automated [data drift tracking](data-drift).\n\n??? faq ""How are missing values handled?""\n\tDataRobot handles missing values differently, depending on the model and/or value type. There are [certain patterns](model-ref#missing-values) recognized and handled as missing, as well as [disguised missing value](quality-check#disguised-missing-values) handling.\n\n## Build models {: #build-models }\n\n??? faq ""Does DataRobot support feature transformations?""\n\tIn AutoML, DataRobot performs [automatic feature transformations](auto-transform) for features recognized as type “date,” adding these new features to the modeling dataset. Additionally, you can create [manual transformations](feature-transforms) and change variable type. For image datasets, the [train-time image augmentation](ttia-lists) process creates new training images. The [time series feature derivation](ts-create-data#create-the-modeling-dataset) process creates a new modeling dataset. [Feature Discovery](fd-overview) discovers and generates new features from multiple datasets to consolidate datasets. Or, use a [Spark SQL query](spark) from the AI Catalog to prepare a new dataset from a single dataset or blend two or more datasets. Transformed features are marked with an info icon on the data page.\n\n??? faq ""Can I choose which optimization metric to use?""\n    The optimization metric defines how to score models. DataRobot selects a metric best-suited for your data from a [comprehensive set of choices](opt-metric), but also computes alternative metrics. After [EDA1](eda-explained#eda1) completes, you can [change the selection](additional#change-the-optimization-metric) from the **Advanced Options > Additional** tab. After [EDA2](eda-explained#eda2) completes, you can redisplay the Leaderboard listing based on a different computed metric.\n\n??? faq ""Can I change the project type?""\n    Once you enter a target feature, DataRobot automatically analyzes the training dataset, determines the project type (classification if the target has categories or regression if the target is numerical), and displays the distribution of the target feature. If the project is classified as regression and [eligible for multiclass conversion](multiclass#change-regression-projects-to-multiclass), you can change the project to a classification project, and DataRobot will interpret values as classes instead of continuous values.\n\n??? faq ""How do I control how to group or partition my data for model training?""\n\tBy default, DataRobot splits your data into a 20% holdout (test) partition and an 80% cross-validation (training and validation) partition, which is divided into five sub-partitions. You can change these values after loading data and selecting a target from the **Advanced Options > Partitioning** tab. From there, you can [set the method](partitioning), sizes for data partitions, number of partitions for cross-validation, and the method by which those partitions are created.\n\n??? faq ""What do the green ""importance"" bars represent on the Data tab?""\n\tThe [Importance green bars](model-ref#importance-score), based on [""Alternating Conditional Expectations""](https://www.jds-online.com/files/JDS-156.pdf) (ACE) scores,  show the degree to which a feature is correlated with the target. Importance has two components&mdash;Value and Normalized Value&mdash;and is calculated independently for each feature in the dataset.\n\n??? faq ""Does DataRobot handle natural language processing (NLP)?""\n\tWhen text fields are detected in your data, DataRobot automatically detects the language and applies appropriate preprocessing. This may include advanced tokenization, data cleaning (stop word removal, stemming, etc.), and vectorization methods. DataRobot supports n-gram matrix (bag-of-words, bag-of-characters) analysis as well as word embedding techniques such as Word2Vec and fastText with both CBOW and Skip-Gram learning methods. Additional capabilities include Naive Bayes SVM and cosine similarity analysis. For visualization, there are per-class word clouds for text analysis. You can see the applied language preprocessing steps in the [model blueprint](blueprints).\n\n\n??? faq ""How do I restart a project with the same data?""\n\tIf your data is stored in the AI Catalog, you can [create and recreate](catalog#create-a-project) projects from that dataset. To recreate a project&mdash;using either just the data or the data and the settings (i.e., to duplicate the project)&mdash;use the [**Actions** menu](manage-projects#duplicate-a-project) in the project control center.\n\n\n??? faq ""Do I have to use the UI or can I interact programmatically?""\n\tDataRobot provides both a UI and a REST API. The UI and REST API provide nearly matching functionality. Additionally, [Python and R clients](https://docs.datarobot.com/en/api/) provide a subset of what you can do with the full API.\n\n??? faq ""Does DataRobot provide partner integrations?""\n\tDataRobot offers an an [Alteryx](alteryx) add-in and a [Tableau](tableau) extension. A [Snowflake integration](fd-overview#snowflake-integration) allows joint users to execute Feature Discovery projects in DataRobot while performing computations in Snowflake for minimized data movement.\n\n=== ""SaaS""\n\t??? faq\t""What is the difference between prediction and modeling servers?""\n\t\tModeling servers power all the creation and model analysis done from the UI and from the R and Python clients. Prediction servers are used solely for making predictions and handling prediction requests on deployed models.\n\n=== ""Self-Managed""\n\t??? faq\t""What is the difference between prediction and modeling servers?""\n\t\tModeling servers power all the creation and model analysis done from the UI and from the R and Python clients. Modeling worker resources are reported in the [Resource Monitor](resource-monitor). Prediction servers are used solely for making predictions and handling prediction requests on deployed models.\n\n## Model insights {: #model-insights}\n\n??? faq ""How do I directly compare model performance?""\n\tThere are many ways to compare model performance. Some starter points:\n\n\t* Look at the Leaderboard to compare [Validation, Cross-Validation, and/or Holdout scores](leaderboard-ref#columns-and-tools).\n\t* Use [Learning Curves](learn-curve) to help determine whether it is worthwhile to increase the size of your dataset for a given model. The results help identify which models may benefit from being trained into the Validation or Holdout partition.\n\t* [Speed vs Accuracy ](speed) compares multiple models in a measure of the tradeoff between runtime and predictive accuracy. If prediction latency is important for model deployment then this will help you find the most effective model.\n\t* [Model Comparison](model-compare) lets you select a pair of models and compare a variety of insights (Lift Charts, Profit Curve, ROC Curves).\n\n??? faq ""How does DataRobot choose the recommended model?""\n\tAs part of the Autopilot modeling process, DataRobot identifies the most accurate non-blender model and [prepares it for deployment](model-rec-process). Although Autopilot recommends and prepares a single model for deployment, you can initiate the Autopilot recommendation and deployment preparation stages for any Leaderboard model.\n\n??? faq ""Why not always use the most accurate model?""\n\tThere could be several reasons, but the two most common are:\n\n\t* Prediction latency&mdash;This means the speed at which predictions are made. Some business applications of a model will require very fast predictions on new data. The most accurate models are often blender models which are usually slower at making predictions.\n\t* Organizational readiness&mdash;Some organizations favor linear models and/or decision trees for perceived interpretability reasons. Additionally, there may be compliance reasons for favoring certain types of models over others.\n\n??? faq ""Why doesn’t the recommended model have text insights?""\n\tOne common reason that text models are not built is because DataRobot removes single-character ""words"" when model building, a common practice in text mining. If this causes a problem, look at your [model log](log) and consider the [documented workarounds](analyze-insights#text-based-insights).\n\n??? faq ""What is model lift?""\n\tLift is the ratio of points correctly classified as positive in a model versus the 45-degree line (or baseline model) represented in the [Cumulative Gain](cumulative-charts#cumulative-gain-chart) plot. The cumulative charts show, for a given % of top predictions, how much more effective the selected model is at identifying the positive class versus the baseline model.\n\n??? faq ""What is the ROC Curve chart?""\n\tThe [ROC Curve](roc-curve-tab/index) tab provides extensive tools for exploring classification, performance, and statistics related to a selected model at any point on the probability scale. Documentation discusses prediction thresholds, the Matthews Correlation Coefficient (MCC), as well as interpreting the ROC Curve, Cumulative Gain, and Profit Curve charts.\n\n??? faq ""Can I tune model hyperparameters?""\n\tYou can tune model hyperparameters in the [Advanced Tuning](adv-tuning) tab for a particular model. From here, you can manually set model parameters, overriding the DataRobot selections. However, consider whether it is instead better to spend “tuning” time doing feature engineering, for example using [Feature Discovery](fd-overview) for  automated feature engineering.\n\n??? faq ""How is Tree-Based Variable Importance different from Feature Impact?""\n\t[Feature Impact](feature-impact) shows, at a high level, which features are driving model decisions. It is computed by permuting the rows of a given feature while leaving the rows of the other features unchanged, and measuring the impact of the permutation on the model\'s predictive performance. [Tree-based Variable Importance](analyze-insights#tree-based-variable-importance) shows how much gain each feature adds to a model--the relative importance of the key features. It is only available for tree/forest models (for example, Gradient Boosted Trees Classifier or Random Forest).\n\n??? faq ""How can I find models that produced coefficients?""\n\tAny model that produces coefficients can be identified on the Leaderboard with a Beta (![](images/icon-cf-export.png)) tag. Those models allow you to [export the coefficients](coefficients#generate-output) and transformation parameters necessary to verify steps and make predictions outside of DataRobot. When a blueprint has coefficients but is not marked with the Beta tag, it indicates that the coefficients are not exact (e.g., they may be rounded).\n\n??? faq ""What is the difference between ""text mining"" and ""word clouds""?""\n\tThe [Text Mining](analyze-insights#text-mining-insights) and [Word Cloud](analyze-insights#word-cloud-insights) insights demonstrate text importance in different formats. *Text Mining* shows text coefficient effect (numeric value) and direction (positive=red or negative=blue) in a bar graph format. The Word Cloud shows the normalized version of those coefficients in a cloud format using text size and color.\n\n??? faq ""Why are there variables in some insights that are not in the dataset?""\n\tDataRobot performs a variety of data preprocessing, such as [automatic transformations](auto-transform) and deriving features (for example, ratios and differences). When building models, it uses all useful features, which includes both original and derived variables.\n\n??? faq ""Why does Feature Effects show missing partial dependence values when my dataset has none?""\n\tPartial dependence (PD) is reported as part of Feature Effects. It shows how dependent the prediction value is on different values of the selected feature. Prediction values are affected by all features, though, not just the selected feature, so PD must measure how predictions change given different values of the other features as well. When computing, DataRobot adds “missing” as one of the values [calculated](feature-effects#partial-dependence-calculations) for the selected feature, to show how the absence of a value will affect the prediction. The end result is the average effect of each value on the prediction, given other values, and following the distribution of the training data.\n\n??? faq ""How do I determine how long it will take to calculate Feature Effects?""\n\tIt can take a long time to compute Feature Effects, particularly if blenders are involved. As a rough estimate of the runtime, use the [Model Info](model-info) tab to check the time it takes, in seconds, for your model to score 1000 rows. Multiply this number by 0.5-1.0 hours. Note that the actual runtime may be longer if you don’t assign enough workers to work on all Feature Effects sub-jobs simultaneously.\n\n??? faq ""Why is a feature’s impact different depending on the model?""\n\tAutopilot builds a wide selection of models to capture varying degrees of underlying complexity and each model has its strengths and weaknesses in addressing that complexity. [A feature’s impact](feature-impact) shouldn\'t be drastically different, however, so while the ordering of features will change, the overall inference is often not impacted. Examples:\n\n\t* A model that is not capable of detecting nonlinear relationships or interactions will use the variables one way, while a model that can detect these relationships will use the variables another way. The result is different feature impacts from different models.\n\t* If two variables are highly correlated, a regularized linear model will tend to use only one of them, while a tree-based method will tend to use both, and at different splits. With the linear model, one of these variables will show up high in feature importance and the other will be low, while with the tree-based model, both will be closer to the middle.\n', '# Version 7.1.2 {: #version-712 }\n\n_August 2, 2021_\n\nThe DataRobot v7.1.2 release includes some fixed issues in the DataRobot Self-Managed AI Platform platform. See the v7.1.0 release notes for:\n\n* [Features introduced in v7.1.0](v7.1.0-aml)\n\n## Updated language localization {: #updated-language-localization }\n\nLocalization of the documentation has been updated to include Japanese content for all new 7.1 features.\n\nTo change languages, open your [user settings](getting-help) and select a language for your session. The selection remains until you reset it.\n\n![](images/languages-set-pt-jp.png)\n\n## Issues fixed in v7.1.2 {: #issues-fixed-in-v712 }\n\nThe following issues have been fixed since Enterprise release v7.1.1:\n\n### Enterprise {: #enterprise }\n\n* EP-1226: Fixes an issue where `patroni` installs failed because nodes were not syncing.\n* EP-1345: Fixes an issue that caused a KeyError during DB migration.\n\n### Platform {: #platform }\n\n* PRODSEC-234: Prevents containers from acquiring new privileges.\n* XAI-4238: Fixes an issue when loading certain models with SHAP explainers that were built in DataRobot version 5.2.2, which failed to load after upgrading to version 6.3.\n\n### Feature Discovery {: #feature-discovery }\n\n* SAFER-3964: Fixes an issue where Feature Discovery projects created before DataRobot version 7.1 resulted in an error when running predictions.\n\n### Time Series {: #time-series }\n\n* TIME-8745: Allows `COMPUTE_TIMESERIES_AOT_THRESHOLD` to be configured from the environment.\n\n_All product and company names are trademarks&trade; or registered&reg; trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them_.\n', '---\ntitle: Use feature engineering and Visual AI with acoustic data\ndescription: Generate image features in addition to aggregate numeric features for high frequency data sources.\n\n---\n\n# Use feature engineering and Visual AI with acoustic data {: #use-feature-engineering-and-visual-ai-with-acoustic-data }\n\n[Access this AI accelerator on GitHub <span style=""vertical-align: sub"">:material-arrow-right-circle:{.lg }</span>](https://github.com/datarobot-community/ai-accelerators/blob/main/use_cases_and_horizontal_approaches/high_frequency_data_classification_using_spectrograms_n_numerics/high_frequency_classification_spectrograms_n_numerics.ipynb){ .md-button }\n\nThe density of high frequency data presents a challenge for standard machine learning workflows that lack specialized feature engineering techniques to condense the signal, extracting and highlighting its uniqueness. DataRobot\'s multimodal input capability supports simultaneously leveraging numerics and images, which for this use-case is particularly beneficial for including descriptive spectrograms that enable you to leverage well-established computer vision techniques for complex data understanding.\n\nThis example notebook shows how to generate image features and aggregate numeric features for high frequency data sources. This approach converts audio wav files from the time domain into the frequency domain to create several types of spectrograms. Statistical numeric features computed from the converted signal add additional descriptors to aid classification of the audio source.\n']"
