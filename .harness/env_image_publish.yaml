pipeline:
  name: env_image_build_and_publish
  identifier: env_image_publish
  projectIdentifier: datarobotusermodels
  orgIdentifier: Custom_Models
  tags: {}
  properties:
    ci:
      codebase:
        connectorRef: account.svc_harness_git1
        repoName: datarobot-user-models
        build: <+input>
  stages:
    - stage:
        name: env_image_build_and_publish
        identifier: env_image_publish
        description: ""
        type: CI
        spec:
          cloneCodebase: true
          execution:
            steps:
              - step:
                  type: Run
                  name: Preinstall maven into java env
                  identifier: Run_1
                  spec:
                    connectorRef: account.dockerhub_datarobot_read
                    image: alpine/git
                    shell: Sh
                    command: |-
                      if [ "<+pipeline.variables.env_name>" = "java_codegen" ] && [ "<+pipeline.variables.env_folder>" != "public_fips_dropin_environments" ]; then
                        # For non FIPS-compliant java env we install maven and build java components to make maven
                        # download most of the dependencies. For the FIPS-compliant java env we don't need to do this
                        # because we skip installations and use the already installed dependencies.
                        cp -r custom_model_runner/ <+pipeline.variables.env_folder>/java_codegen/
                        cd <+pipeline.variables.env_folder>/java_codegen
                        echo -e "RUN apt-get update && apt-get install -y maven" >> Dockerfile 
                        echo -e "COPY ./custom_model_runner /tmp/custom_model_runner" >> Dockerfile 
                        echo -e "RUN cd /tmp/custom_model_runner && make java_components" >> Dockerfile 
                      fi
              - step:
                  type: BuildAndPushDockerRegistry
                  name: BuildAndPushDockerRegistry_1
                  identifier: BuildAndPushDockerRegistry_1
                  spec:
                    connectorRef: datarobot_user_models_read_write
                    repo: datarobotdev/datarobot-user-models
                    tags:
                      - <+pipeline.variables.image_tag>
                    dockerfile: <+pipeline.variables.env_folder>/<+pipeline.variables.env_name>/Dockerfile
                    context: <+pipeline.variables.env_folder>/<+pipeline.variables.env_name>
                    resources:
                      limits:
                        memory: 3G
          caching:
            enabled: false
            paths: []
          platform:
            os: Linux
            arch: Amd64
          runtime:
            type: Cloud
            spec: {}
  variables:
    - name: env_folder
      type: String
      description: ""
      required: true
      value: <+input>.allowedValues(public_dropin_environments, public_fips_dropin_environments)
    - name: env_name
      type: String
      description: ""
      required: true
      value: <+input>.allowedValues(python3_sklearn,python3_xgboost,python3_onnx,python3_pytorch,python3_keras,python3_pmml,python311_genai,python39_genai,java_codegen,r_lang,python311)
    - name: image_tag
      type: String
      description: ""
      required: true
      value: <+input>.default(<+pipeline.variables.env_folder>_<+pipeline.variables.env_name>_latest)
