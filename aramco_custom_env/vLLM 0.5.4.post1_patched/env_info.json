{
  "id": "662d6a54ef58f64c5a07d122",
  "name": "[GenAI] vLLM Inference Server (0.4.2)",
  "description": "A high-throughput and memory-efficient inference and serving engine for LLMs (patched).",
  "programmingLanguage": "python",
  "environmentVersionId": "67002064953c904b1628d341",
  "isPublic": true
}
