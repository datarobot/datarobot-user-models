{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4552ef1-39b6-4b32-bb8d-9df481c9d5e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a Custom Task\n",
    "\n",
    "This notebook walks through the general workflow of building a custom task using python classes. We'll also demonstrate how to deploy your custom task to DataRobot and integrate it into an existing blueprint!\n",
    "\n",
    "## Note\n",
    "The later sections of this tutorial require that you have access to Cloud DataRobot (app.datarobot.com or app.eu.datarobot.com)\n",
    "\n",
    "## Agenda\n",
    "In this tutorial, we'll learn:\n",
    "1. How to create a custom task using simple python classes\n",
    "2. How to test your python class\n",
    "3. How to use the drum cli tools to test out your custom task \n",
    "4. How to use the DataRobot API to deploy your custom task to the DataRobot cloud for use in projects\n",
    "5. How to insert a custom task on the DataRobot cloud into a blueprint\n",
    "\n",
    "## Setup and Requirements\n",
    "\n",
    "1. Ensure you have the DataRobot datarobot-user-models (DRUM) downloaded: https://github.com/datarobot/datarobot-user-models\n",
    "2. Navigate to the drum repo, create a new virtual environment, and pip install -r requirements.txt from that repo\n",
    "3. Follow the installation guide here for the blueprint workshop: https://blueprint-workshop.datarobot.com/setup.html \n",
    "\n",
    "Note: all filepaths are relative to the top level directory of the datarobot-user-models above. If you are in the correct directory you should see folders \n",
    "like task_templates, public_dropin_environments, tests, etc. \n",
    "\n",
    "\n",
    "## Configuring Models and Environments\n",
    "For more information on how to properly configure custom tasks and environments, read the README of our [DataRobot User Models repository](https://github.com/datarobot/datarobot-user-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838b9ec-24b8-4307-b5e7-042ae8f0ba5d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a custom task\n",
    "\n",
    "First, we need to import a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2e2f39ff-8382-494d-a085-e9d69515c84d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "import keras.models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "26180fa4-85f4-470c-ad85-ea83ac0fabbe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's build a neural network! First we'll lay out the code, then we'll walk through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "19999210-29c2-47a4-81a0-6625f173cd1a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from datarobot_drum.custom_task_interfaces import RegressionEstimatorInterface\n",
    "\n",
    "class CustomTask(RegressionEstimatorInterface):\n",
    "    \n",
    "    def fit(self, X, y, row_weights=None, **kwargs):\n",
    "        tf.random.set_seed(1234)\n",
    "        input_dim, output_dim = len(X.columns), 1\n",
    "\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Dense(\n",
    "                    input_dim, activation=\"relu\", input_dim=input_dim, kernel_initializer=\"normal\"\n",
    "                ),\n",
    "                Dense(input_dim // 2, activation=\"relu\", kernel_initializer=\"normal\"),\n",
    "                Dense(output_dim, kernel_initializer=\"normal\"),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "        callback = EarlyStopping(monitor=\"loss\", patience=3)\n",
    "        model.fit(\n",
    "            X, y, epochs=20, batch_size=8, validation_split=0.33, verbose=1, callbacks=[callback]\n",
    "        )\n",
    "\n",
    "        # Attach the model to our object for future use\n",
    "        self.estimator = model\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def save(self, artifact_directory):\n",
    "\n",
    "        # If your estimator is not pickle-able, you can serialize it using its native method,\n",
    "        # i.e. in this case for keras we use model.save, and then set the estimator to none\n",
    "        keras.models.save_model(self.estimator, Path(artifact_directory) / \"model.h5\")\n",
    "\n",
    "        # Helper method to handle serializing, via pickle, the CustomTask class\n",
    "        self.save_task(artifact_directory, exclude=[\"estimator\"])\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, artifact_directory):\n",
    "        \n",
    "        # Helper method to load the serialized CustomTask class\n",
    "        custom_task = cls.load_task(artifact_directory)\n",
    "        custom_task.estimator = keras.models.load_model(Path(artifact_directory) / \"model.h5\")\n",
    "\n",
    "        return custom_task\n",
    "    \n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        # Note how the regression estimator only outputs one column, so no explicit column names are needed\n",
    "        return pd.DataFrame(data=self.estimator.predict(X))\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7aa9b5af-854b-4a8f-b174-6c58b7fa005e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There's a lot above, but the key idea is that we have 4 hooks: fit, save, load, and predict. DataRobot will use these hooks automatically to run our custom task. As you can probably guess, these hooks run in a specific order: first we train (fit) a model, then we serialize it (see the section [below]), then we load (i.e. deserialize) it again, and finally we make predictions. \n",
    "\n",
    "One thing to note is that the above CustomTask is simply a python class, which means we can also add helper methods or have functions / classes in a helper file that we import. The more complex your CustomTask, the more it probably makes sense to import a separate helper file to keep things simple. See [here] for an example of directly using helper methods in the class and [here] for using a separate helper file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d7dfa-e1d9-4ac7-bbd8-0f31e99db09b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training our model with Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016bfbc-dc73-4f2e-a936-d6db6e4ccef7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's actually use the class above. Since this is an ordinary python class, all we need to do is build an object and we can test it out to ensure our methods work! First, let's grab a dataset and then separate out the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e8723510-6c27-4ece-8bfc-0e28921fef1f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df = pd.read_csv(\"tests/testdata/juniors_3_year_stats_regression.csv\")\n",
    "\n",
    "y = df['Grade 2014']\n",
    "X = df.drop(labels=['Grade 2014'], axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "14e4c8c8-110d-464d-834b-c6560d230625",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1b4a1f57-c912-4ced-9bbe-fa7ab1cbc2c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "task = CustomTask()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "82989943-c38f-403f-bc08-27b8370d59b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "task = task.fit(X,y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f14e29fc-b5a3-4448-a9c5-f521f8c8244d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving and Loading our Custom Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a889669-2be2-403d-985b-f62c4a61a9ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You may be wondering why we need to save our model only to immediately load it again to make predictions. The reason is that model training and prediction, also known as inference or scoring, are distinct tasks that may have very different resource requirements. As an extreme example, one of DataRobot's proprietary models is a genetic algorithm known as Eureqa. Training this model can take some time, as it iterates through mathematical transformations. However the output of this model is a simple mathematical equation, which can run almost instantly on very modest computational resources. So during training we want to allocate a high amount of compute, but while the model is making predictions, e.g. it is deployed and waiting to receive new data, we want a far lower amount of compute allocated.  \n",
    "\n",
    "The way we achieve this is by using Docker containers, which we can think of as extremely lightweight virtual machines. This allows our training container to have significantly different computational resources allocated than our prediction container. But since the training and prediction steps are in separate containers, we need a way to move trained models and other useful artifacts, e.g. class labels, between them. The solution is to write out the artifacts to disk, i.e. serialize them. So our save method at the end of training will write out the model to a shared file storage location and then our load method at the beginning of making predictions will read the artifacts into memory, i.e. deserialize them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ea43b8ec-80f9-4b99-8eaf-f778aba66225",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "task.save(\".\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d5419aaa-ab0e-4bd9-bd54-918af84a35d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "task = task.load(\".\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0ffeebdf-403e-4efb-a40c-4fc92af07c34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "task.predict(X)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d12611cf-00f2-420c-b237-6ce09111cf45",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look more deeply at the save method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "69f5d9ff-a732-4b4b-982b-3b7c98b543b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "??task.save"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "14df8a48-7dae-40ee-b007-16fcaa28bd38",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see, we use two distinct functions to save our model. First, we use the keras function save_model to save our self.estimator, i.e. the trained model from the fit function. Then we use a built in helper function, save_task. Let's look at save task quickly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50e28c39-8f44-4ffb-a86c-6d74fe9008a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "??task.save_task"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7d003172-6a15-4a36-8f89-f39da5200894",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Don't worry about understanding every line above. The key point is that we set everything passed in the exclude parameter to None, then we use a the standard python library pickle to serialize the CustomTask object. The reason we do this is flexibility. There are a wide array of python ML frameworks, e.g. keras / tensorflow, pytorch, xgboost, etc. Many of these frameworks, particularly those around neural networks, have their own serialization functions that handle all the complexities around storing weights, archiectures, etc. \n",
    "\n",
    "So the recommended pattern is to save your estimator using your framework's serialization function, e.g. keras.models.save_model above, and then use the helper function save_task we provide to save the rest of your CustomTask object. \n",
    "\n",
    "If we look at the load method, we see that we simply reverse the order. First we use the helper function load_task to load our CustomTask object using pickle, then we load our estimator into self.estimator using the keras function load_model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b8a6eb6-8bb5-4cbc-b0c0-659b3069c107",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "??task.load"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7bb8e275-f42a-42a4-b814-b70f1247223b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It's important to understand that some python ML frameworks, notably Sklearn, use pickle for their serialization as well. This means we don't need to write our own save / load functions in our CustomTask, as the default functions will simply pickle everyting including the model. The below example is from the template [here] and shows how this looks for a simple sklearn model. You can see that the whole CustomTask is around 10 lines of code. Pretty neat, huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "961306ce-2a76-4f6a-b229-eb468507726d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class CustomTask(RegressionEstimatorInterface):\n",
    "    def fit(self, X, y, row_weights=None, **kwargs):\n",
    "        \n",
    "        self.estimator = Ridge()\n",
    "        self.estimator.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "\n",
    "        return pd.DataFrame(data=self.estimator.predict(X))\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ff725e50-b847-471e-9d47-34f7dd1cdc72",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Making Predictions with the Correct Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98816191-ba64-433d-a11d-a3d5752bc405",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A CustomTask currently has to output a pandas dataframe where the rows are the samples and the columns the predictions. \n",
    "\n",
    "As you may have noticed, all of examples so far have been regressors, i.e. outputting a single, numeric prediction. So our rows are just the number of samples and we have a single column (which doesn't need a specific name). We can see that our CustomTasks above inherit from the RegressionEstimatorInterface, which enforces this behavior.\n",
    "\n",
    "We can use the exact same behavior when we are creating an anomaly detection CustomTask, because the output is again a single numeric column representing how anomalous each sample is. There is a corresponding AnomalyEstimatorInterface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c045f-98f3-4a0b-8d5a-c5566b426c52",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Things are a little trickier though when we need to create a binary or multiclass estimator. That's because we'll need to align the columns to the classes. Keep in mind that our CustomTask will run inside a DataRobot blueprint, which will be given a list of classes in the target. Let's take a look at an example binary estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f98e0b28-99ed-4f50-9cec-9f174c5b9300",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from datarobot_drum.custom_task_interfaces import BinaryEstimatorInterface\n",
    "\n",
    "class CustomTask(BinaryEstimatorInterface):\n",
    "    def fit(self, X, y, row_weights=None, **kwargs):\n",
    "        \n",
    "        self.estimator = DecisionTreeClassifier()\n",
    "        self.estimator.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, **kwargs):\n",
    "\n",
    "        # Note that binary estimators require two columns in the output, the positive and negative class labels\n",
    "        # So we need to pass in the class names derived from the estimator as column names OR\n",
    "        # we can use the class labels from DataRobot stored in\n",
    "        # kwargs['positive_class_label'] and kwargs['negative_class_label']\n",
    "        return pd.DataFrame(data=self.estimator.predict_proba(X), columns=self.estimator.classes_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "72145ce3-a5a3-4a79-a560-a6bd2dd06d66",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first thing to notice is that because we're outputting probabilities, we define a predict_proba instead of a predict function. The second thing to notice is that we have to provide the column names of our dataframe, and they have to align to the classes of our dataset. If you look at the fit function, you'll notice we directly pass in the target column y. This will have our target labels and these will be passed to our model as we train it, i.e. self.estimator.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87859af7-9def-4eac-b5e4-c85746cd330f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For binary classification, DataRobot requires there to be 2 columns: the positive class prediction and the negative class prediction \n",
    "(which is the inverse). Obviously, these two numbers should sum up to 1.0. \n",
    "\n",
    "For frameworks that output 2 classes, like sklearn, we can simply use the classes stored by the sklearn model itself, i.e. self.estimator.classes_\n",
    "\n",
    "Some frameworks, such as pytorch, instead only output one column (typically the positive class probability). In those cases we have to derive the negative class column, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87d006f7-71d1-46ef-ba53-55c5fdea8824",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "    def predict_proba(self, X, **kwargs):\n",
    "        \"\"\"Since pytorch only outputs a single probability, i.e. the probability of the positive class,\n",
    "         we use the class labels passed in kwargs to label the columns\"\"\"\n",
    "        data_tensor = torch.from_numpy(X.values).type(torch.FloatTensor)\n",
    "        predictions = self.estimator(data_tensor).cpu().data.numpy()\n",
    "        \n",
    "        predictions = pd.DataFrame(predictions, columns=[kwargs[\"positive_class_label\"]])\n",
    "\n",
    "        # The negative class probability is just the inverse of what the model predicts above\n",
    "        predictions[kwargs[\"negative_class_label\"]] = (\n",
    "            1 - predictions[kwargs[\"positive_class_label\"]]\n",
    "        )\n",
    "        return predictions\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2528c5b1-5bbd-4c02-878b-11d9c9f1cee2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Multiclass is slightly more challenging. Here we'll need to output the probability of each class as a separate column. If our framework stores. the classes, like many sklearn models, we can use the same exact same approach as above with binary classification. If the framework doesn't store the classes, e.g. pytorch, then we'll need to store the classes during the fit step on the self object so it can be used as the columns (Note: the save & load methods are excluded below to focus in on the unique aspects of multiclass):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9a46774-4c5d-46d6-995e-2cf423ebc690",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from datarobot_drum.custom_task_interfaces import MulticlassEstimatorInterface\n",
    "\n",
    "\n",
    "class CustomTask(MulticlassEstimatorInterface):\n",
    "    def fit(self, X, y, row_weights=None, **kwargs):\n",
    "        \"\"\"Note how we encode the class labels and store them on self to be used in the predict hook\"\"\"\n",
    "        self.lb = LabelEncoder().fit(y)\n",
    "        y = self.lb.transform(y)\n",
    "\n",
    "        # For reproducible results\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        self.estimator, optimizer, criterion = build_classifier(X, len(self.lb.classes_))\n",
    "        train_classifier(X, y, self.estimator, optimizer, criterion)\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        \"\"\"Note how the column names come from the encoded class labels in the fit hook above\"\"\"\n",
    "        data_tensor = torch.from_numpy(X.values).type(torch.FloatTensor)\n",
    "        predictions = self.estimator(data_tensor).cpu().data.numpy()\n",
    "\n",
    "        # Note that multiclass estimators require one column per class in the output\n",
    "        # So we need to pass in the class names derived from the estimator as column names.\n",
    "        return pd.DataFrame(data=predictions, columns=self.lb.classes_)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ff386820-c309-4d99-9243-2e51272817fa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformers vs. Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc84954-0660-46e4-9263-8ca4b4aca0b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So far we've focused on Estimators, which output a prediction. We can also create transforms, which manipulate the data and pass it along to (eventually) an estimator. Note: this final estimator can either be a built in DataRobot task or a CustomTask.\n",
    "\n",
    "The key difference between estimators and transforms is that instead of a predict function we have a transform function. The transform function also returns a dataframe, but instead of predictions it returns the transformed data. Note that while you can certainly create or remove columns, the transform function must output the same number of rows, e.g. you can't create new synthetic data.\n",
    "\n",
    "It's important to keep in mind that the fit and transform functions also run in separate containers. So any artifacts need to be saved, either by using the default pickling functions that run automatically, or by defining your own save and load functions. \n",
    "\n",
    "Below is a simple example where you fill in missing values using median imputation. Note that because we are simply storing the imputation function as part of self, we can use the built in save and load functions so we don't have to define our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45a3492b-d926-439b-8653-e56622cd28a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from datarobot_drum.custom_task_interfaces import TransformerInterface\n",
    "\n",
    "\n",
    "class CustomTask(TransformerInterface):\n",
    "    def fit(self, X, y, **kwargs):\n",
    "\n",
    "        # Any information derived from the training data (i.e. median values for each column) should be stored to self.\n",
    "        # Then, in the transform hook below, we use this information to transform any data that passes through this task\n",
    "        self.medians = X.median(axis=0, numeric_only=True, skipna=True).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        return X.fillna(self.medians)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c540cc1a-0c9f-42d2-afa0-10e672f4ee29",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One important thing to note is that we provide the target column as part of the fit function above so you can create target encoders or other related transforms. Since your CustomTask transform will run in a DataRobot blueprint, DataRobot will automatically handle setting up the defined partitioning strategy, e.g. 5 fold cross validation, and only run the fit function on the training folds. But it's always possible to introduce target leakage / overfitting with any encoding strategy, so please take care!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45714758-be06-4e9a-9bd2-1fce3c82a55e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Understanding the CustomTask Interface (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a1495a-db5e-4a14-90cf-8a09d26cf2f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the above sections, we mentioned that DataRobot automatically serialize your CustomTask as a pickle file unless you provided your own save / load functions. This is because we defined a series of interfaces that inherit from each other and have default implementations. Let's walk through them quickly.\n",
    "\n",
    "The top level interface is the target type interfaces, e.g. TransformerInterface, MulticlassEstimatorInterface, etc. These provide the relevant predict functions. Each of these inherits from a CustomTaskInterface, which provides the default fit method. \n",
    "\n",
    "Finally, the top level function is. the Serializable interface. You can see that by default, the save and load functions simply call the save_task and load_task helper functions. This is to provide a level of abstraction. When you define your own CustomTask save or load function, you can use the save_task and load_task functions to avoid having to write your own code to pickle the CustomTask object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "238fedae-7ea1-41f8-be82-2d4f76663c3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Serializable(object):\n",
    "    default_artifact_filename = \"drum_artifact.pkl\"\n",
    "\n",
    "    def save(self, artifact_directory):\n",
    "\n",
    "        self.save_task(artifact_directory)\n",
    "\n",
    "        # For use in easy chaining, e.g. CustomTask().fit().save().load()\n",
    "        return self\n",
    "\n",
    "    def save_task(self, artifact_directory, exclude=None):\n",
    "        \"\"\"\n",
    "        Helper function that abstracts away pickling the CustomTask object. It also can\n",
    "        automatically set previously serialized variables to None, e.g. when using keras you likely\n",
    "        want to serialize self.estimator using model.save() or keras.models.save_model() and then\n",
    "        pass in exclude='estimator'\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        artifact_directory: str\n",
    "            Path to the directory to save the serialized artifact(s) to.\n",
    "        exclude: List[str]\n",
    "            Variables on the CustomTask object we want to exclude from serialization by setting to None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # If any custom task variables are excluded in the pickle, temporarily store them here, set them to None, then\n",
    "        # restore them back onto the class after serialization\n",
    "        variables_to_restore = {}\n",
    "\n",
    "        if exclude:\n",
    "            for custom_task_variable in exclude:\n",
    "                try:\n",
    "                    # Ensure the variable actually exists in the custom task\n",
    "                    variables_to_restore[custom_task_variable] = getattr(self, custom_task_variable)\n",
    "                except AttributeError:\n",
    "                    raise DrumCommonException(\n",
    "                        f\"The object named {custom_task_variable} passed in exclude= was not found\"\n",
    "                    )\n",
    "\n",
    "                # Set it to None so it does not get serialized\n",
    "                setattr(self, custom_task_variable, None)\n",
    "        with open(\n",
    "            os.path.join(artifact_directory, Serializable.default_artifact_filename), \"wb\"\n",
    "        ) as fp:\n",
    "            pickle.dump(self, fp)\n",
    "\n",
    "        for custom_task_variable, value in variables_to_restore.items():\n",
    "            setattr(self, custom_task_variable, value)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, artifact_directory):\n",
    "\n",
    "        return cls.load_task(artifact_directory)\n",
    "\n",
    "    @classmethod\n",
    "    def load_task(cls, artifact_directory):\n",
    "        \"\"\"\n",
    "        Helper method to abstract deserializing the pickle object stored within `artifact_directory` and\n",
    "        returning the custom task. Any variables that were excluded in `save_task` must be manually loaded\n",
    "        proceeding this function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        artifact_directory: str\n",
    "            Path to the directory to save the serialized artifact(s) to.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cls\n",
    "            The deserialized object\n",
    "        \"\"\"\n",
    "        with open(\n",
    "            os.path.join(artifact_directory, Serializable.default_artifact_filename), \"rb\"\n",
    "        ) as fp:\n",
    "            deserialized_object = pickle.load(fp)\n",
    "\n",
    "        if not isinstance(deserialized_object, cls):\n",
    "            raise DrumCommonException(\n",
    "                \"load_task method must return a {} class\".format(cls.__name__)\n",
    "            )\n",
    "        return deserialized_object"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "726af462-9065-4c1e-a248-bbc4eb997326",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing the Custom Task "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d984337-fe11-470d-97cc-6e9c2c406c63",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because our CustomTask is simply a python class, we can utilize standard python testing libraries / frameworks. \n",
    "\n",
    "E.g. we can verify with a test dataset that our CustomTask matches the expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6ffdf3e7-928f-4f14-9258-631339228d1a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Note: for this example we're simply checking the outputs against itself. But in reality this would be a list of previous predictions\n",
    "previous_predictions = task.predict(X).values\n",
    "assert all(task.predict(X).values == previous_predictions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "78c16fef-b069-47f2-8c87-544b54a52b4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also make assertions about our output to ensure our predictions are in a sensible range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "520123e9-7651-4ae6-ac69-5ab57deea65e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "maximum_prediction = 50\n",
    "assert task.predict(X).values.max() < maximum_prediction"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "544830e3-ee18-4f32-aa0e-f3ace55a799e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These tests can easily be incorporated into a CI/CD pipeline and run automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014a38c-5dbc-4fc9-a605-abb31de2e673",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# How to upload a Custom Task via the web application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c7bd9-5dfa-4b72-9973-ef8aa33dc70d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The easiest way to upload your custom task is to place all of your code into a file called custom.py. For practice, you can copy the code in the CustomTask above (the Keras regression estimator near the top) and place it in a custom.py. Note that [here] is a example of this exact Keras Regressor Custom Task with all the necessary files if you want to inspect it. \n",
    "\n",
    "The next step is to define a model-metadata.yaml. This tells DataRobot a bit about your custom task, e.g. if it's a binary or regression estimator, and also provides guardrails about what type of data it accepts as input and output. E.g. our Keras estimator above only handles numeric data and can't handle missing values. \n",
    "\n",
    "Finally, you may need to create a requirements.txt file. As mentioned above, your CustomTask is run in a Docker Container. DataRobot by default provides a series of drop in environments for each language / framework (see [here]). In the case of the Keras regression CustomTask above, we don't need to import anything outside what that environment contains. You can see an example of a CustomTask using a requirements.txt file [here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b62112-57d4-4ce0-a07d-d0597d67779d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once you have all the files ready, you can follow instructions [here] to upload your CustomTask through the Model Registry -> Tasks page. Then you can follow instructions [here] to use your custom task in a DataRobot blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6aac02-0d78-4945-9806-d580877785f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# How to upload a Custom Task through the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afa616-6e7c-4d4b-bc50-fedcd828e75e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "DataRobot supports a robust python, R, and REST API that you use to programatically control data prep, modeling, and deployments. The goal is to let you create an end to end model pipeline without ever having to leave your favorite IDE or notebook (although please at least look at our web application, it's pretty!) \n",
    "\n",
    "In the next few sections we'll show how to quickly create a project to use as an example. We'll then run a few of DataRobot's default blueprints so we can add our CustomTask to them. Then we'll upload the CustomTask we created above, visualize the existing blueprint, and finally add our custom blueprint to the task. \n",
    "\n",
    "Please keep in mind the focus here is on CustomTasks and not a general walkthrough of the API. For that, please see the excellent tutorials [here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668c8b8-ae0f-40e0-bee5-e8b4fecdccd0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create and run a project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f12be-f7a7-4145-b244-b087d493e2f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll be using the docs here if you'd like more information: https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.27.1/reference/modeling/project.html#create-a-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e949628d-78cb-45b2-a4dd-cd4a1253bb03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import datarobot as dr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efbd8a80-571f-4f39-a009-5fbf2378ffd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Note, we can also use dr.Dataset.create_from_in_memory_data to create our dataset directly from the pandas dataframe above\n",
    "dataset = dr.Dataset.create_from_file(file_path=\"tests/testdata/juniors_3_year_stats_regression.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d0fb99a-2f86-46ba-9e4c-8b912fa87028",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Note: we could also have just used dr.Project.create to directly create the project. \n",
    "project = dr.Project.create_from_dataset(dataset.id, project_name=\"Keras Regression Custom Task Tutorial\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f89a6a87-e64e-42cf-b777-27afd22b657b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# This will start the modeling, note that it will run asynchronously \n",
    "target_name = \"Grade 2014\"\n",
    "project.analyze_and_model(target=target_name,\n",
    "                   worker_count=-1, # Max workers\n",
    "                   mode=dr.AUTOPILOT_MODE.QUICK\n",
    "                  )\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91ec0a77-de76-43fe-ac7c-ff27174bddcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# This will let us know when autopilot is finished\n",
    "project.wait_for_autopilot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c21316f-7835-4ab1-acf9-08727373cfcb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# We can now list the top 3 models that ran\n",
    "models = project.get_models()\n",
    "print(models[:3])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e45f560c-4b02-460c-a8da-16d7b571438a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Upload our Custom Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6c143-5425-48c0-a428-3eef225613f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have a project with models to work with, let's upload our CustomTask! \n",
    "We'll be using the docs here: https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.27.1/reference/modeling/spec/custom_task.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc84db6-a310-4cf5-be96-fa6e55c911df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first thing we'll need to do is ensure we have the appropriate execution environment for our CustomTask. You can find a list of these in the github [here]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "31bc4380-db44-438a-8d0a-6ce52c0e7e2e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "execution_environment = dr.ExecutionEnvironment.create(\n",
    "    name=\"Python3 Keras Environment\",\n",
    "    description=\"This environment contains Python3 keras library.\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3d99e06-a85f-44d7-9276-83fcc1e5114c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Note this may take a few minutes to build the Docker image\n",
    "execution_environment_version = dr.ExecutionEnvironmentVersion.create(execution_environment.id, \n",
    "                                                                      \"public_dropin_environments/python3_keras\", \n",
    "                                                                      \"First Version\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf7bd20-d7f4-4b1d-b724-be325b38434a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can create a CustomTask and a CustomTaskVersion. We use versions because it's likely will iterate on our CustomTask, \n",
    "and DataRobot by default will use the default version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dfedc99-8a50-433b-b5bb-bcdf558db7ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from datarobot.enums import CUSTOM_TASK_TARGET_TYPE\n",
    "\n",
    "keras_regression = dr.CustomTask.create(\n",
    "    name=\"keras custom task regressor\",\n",
    "    target_type=CUSTOM_TASK_TARGET_TYPE.REGRESSION\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5582354-d697-4073-9a45-d253a4e316ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "custom_task_folder = \"tests/fixtures/custom_task_interface_keras_regression\"\n",
    "\n",
    "task_version = dr.CustomTaskVersion.create_clean(\n",
    "    custom_task_id=keras_regression.id,\n",
    "    base_environment_id=execution_environment.id,\n",
    "    folder_path=custom_task_folder,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36dfd987-1f34-437a-82dc-42845b55c689",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "keras_regression.refresh() # We have to perform a GET request to update\n",
    "keras_regression.latest_version"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8a78a09f-5816-4113-8647-f4b7980faab9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize an existing Blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912043dc-cc61-4cf8-8b0c-1585b58ad54a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we'll use the blueprint workshop API: https://blueprint-workshop.datarobot.com/getting_started.html\n",
    "\n",
    "Please make sure you've installed the appropriate dependencies, notably graphviz (see https://blueprint-workshop.datarobot.com/setup.html) \n",
    "\n",
    "The key point to remember is that DataRobot's Blueprints (BPs) are Directed Acyclic Graphs (DAGs). This means that data flows from left to right. Typically the data is split by data types and fed to different paths or branches that apply preprocessing. Our dataset only has numeric data, so that's all we see. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbd998-c743-49d4-91de-a2b2094015eb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's visualize an exisitng blueprint that was created by DataRobot's AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0e386c52-9986-429e-9db7-e2aeec801eaf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from datarobot_bp_workshop import Workshop, Visualize\n",
    "w = Workshop()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3afea1c9-3aa6-42c0-b663-2526aec4f8eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# One important note is that models and blueprints are separate. A blueprint trained on a dataset becomes a model\n",
    "\n",
    "# This will default to getting the top model \n",
    "models = project.get_models()\n",
    "top_model = models[5] # This is for convenience, you can choose any model you'd like\n",
    "blueprint = dr.Blueprint.get(project.id, top_model.blueprint_id)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5e4111f6-d8e4-4a11-b033-768270d17a3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: if you're comparing to the DataRobot Web Application UI, make sure to click \"Copy and Edit\" otherwise you might see a different visual.\n",
    "This is because the blueprint shown on the leaderboard automatically removes from the visualization any pathways that aren't used. In this case the leaderboard UI will show only the Numeric Variables path because our dataset only has numerics. But the same blueprint run on Numerics + Text data would show both pathways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fc06c683-ce82-4f30-aeca-8cf6e12fc48c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Visualize.show_dr_blueprint(blueprint) "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e955f8b5-722f-4ec0-916c-a245b5cbb042",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can inspect any blueprint to understand what tasks it has. This will give us information if we want to create a brand new blueprint using our CustomTask or insert our CustomTask into an existing blueprint and then modify it further with DataRobot's tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "991d9baa-7005-41f3-b71b-d256eb040766",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "blueprint_graph = w.clone(top_model.blueprint_id, project.id)\n",
    "source_code = blueprint_graph.to_source_code(to_stdout=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "72c2d748-2e49-4591-b0b0-e326fdaa4a82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also easily list all the tasks, see https://blueprint-workshop.datarobot.com/examples/walkthrough/Walkthrough.html#Help-with-Tasks for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a4083-3a3c-40dc-a4fc-a0619e4f2499",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve Custom Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "14580243-7f64-49c3-99a0-13ac6fc9b238",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "w.search_tasks(\"keras custom task regressor\") # the name of our task above"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d43845c9-da52-4c44-9332-ef1b2839ee8c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Add our CustomTask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58fcef-ffd1-43d4-be97-24b69fab4919",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have our top blueprint, let's replace the Keras Neural Network estimator with our own CustomTask! We can grab the source code from the blueprint above, but we'll replace the existing keras estimator with our custom task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5e530da0-0ff5-4924-b037-ea4672945370",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Copied from above\n",
    "w = Workshop(user_blueprint_id='6202e28f1255e7118a50d079')\n",
    "\n",
    "pdm3 = w.Tasks.PDM3(w.TaskInputs.CAT)\n",
    "pdm3.set_task_parameters(cm=50000, sc=10)\n",
    "\n",
    "ndc = w.Tasks.NDC(w.TaskInputs.NUM)\n",
    "\n",
    "rdt5 = w.Tasks.RDT5(ndc)\n",
    "\n",
    "ptm3 = w.Tasks.PTM3(w.TaskInputs.TXT)\n",
    "ptm3.set_task_parameters(d2=0.8, mxf=20000, d1=1, n='l1')\n",
    "\n",
    "ndc_1 = w.Tasks.NDC(w.TaskInputs.DATE)\n",
    "\n",
    "rst = w.Tasks.RST(ndc_1)\n",
    "rst.set_task_parameters(spt=1)\n",
    "\n",
    "# This is the only difference, we replace the KERASR with our Custom Task\n",
    "kerasr = w.CustomTasks.CUSTOMR_6202d89c1255e7118a50d059(rdt5, rst, pdm3, ptm3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b3d075a1-ef02-4347-b6eb-d7204b012c86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "keras_custom_task_blueprint = w.BlueprintGraph(kerasr, name='Keras Custom Task')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "84608d81-e736-457f-bfb7-8eec1ec8c5a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "keras_custom_task_blueprint.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a24ec610-8543-421f-ac46-f20e4ef67fa2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Note this is asynchronous\n",
    "keras_custom_task_blueprint.train(project_id=project.id)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d551510-cb1e-4f07-b41a-790d7fd70777",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Congratulations! You've now successfully created a CustomTask, uploaded it to DataRobot, and inserted it into a Blueprint all within a Jupyter Notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drum_1.6.6",
   "language": "python",
   "name": "drum_1.6.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
