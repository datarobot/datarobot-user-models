{
  "id": "661516a0d53a8a537360dbd9",
  "name": "[NVIDIA] Triton Inference Server",
  "description": "Triton Inference Server is open source software that lets teams deploy trained AI models from any framework, from local or cloud storage and on any GPU- or CPU-based infrastructure in the cloud, data center, or embedded devices.\n\nThis is the standard py3 image with support for Tensorflow, PyTorch, TensorRT, ONNX and OpenVINO models.",
  "programmingLanguage": "python",
  "label": "v25.01-py3+dr.1",
  "environmentVersionId": "67abbb30cbce021ed2502582",
  "environmentVersionDescription": "Security updates.\nFROM nvcr.io/nvidia/tritonserver:25.01-py3",
  "isPublic": true,
  "useCases": [
    "customModel"
  ],
  "imageRepository": "env-gpu-triton-server"
}
