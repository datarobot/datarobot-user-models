name: vLLM Llama3.1-8B Example
type: inference
targetType: textgeneration

runtimeParameterDefinitions:
  - fieldName: HuggingFaceToken
    type: credential
    credentialType: api_token
    description: |-
      Access Token from HuggingFace (https://huggingface.co/settings/tokens). Please make sure
      your account has access to the Llama3.1 family of models.

  - fieldName: max_model_len
    type: numeric
    defaultValue: 10048
    minValue: 0
    description: |-
      Model context length. If unspecified (or set to zero), will be automatically derived from the model config.
      The default value set in this example has been tuned to support GPU-L resource bundle.

  - fieldName: gpu_memory_utilization
    type: numeric
    defaultValue: 0.9
    minValue: 0
    maxValue: 1
    description: |-
      The fraction of GPU memory to be used for the model executor, which can range from 0 to 1.
      For example, a value of 0.5 would imply 50% GPU memory utilization

  - fieldName: max_tokens
    type: numeric
    defaultValue: 4096
    minValue: 1
    description: Max number of symbols in response (only applicable when using legacy predAPI).

  - fieldName: system_prompt
    type: string
    description: |-
      Instructions to the model, to set the tone of model completions (only applicable when
      using legacy predAPI).

  - fieldName: prompt_column_name
    type: string
    defaultValue: promptText
    description: |-
      Column with user's prompt; each row is a separate completion request (only applicable
      when using legacy predAPI).
