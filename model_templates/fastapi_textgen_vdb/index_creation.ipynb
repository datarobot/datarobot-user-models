{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Index\n",
    "\n",
    "This jupyter notebook creates a FAISS based index consisting of docstrings of [SciKit Learn]() esitmators. This index is then used to simulate how vector database and LLM models can be used and integrated into DataRobot. \n",
    "\n",
    "As a first step, let's gather all of the docstrings from Scikit-Learn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import faiss\n",
    "from sklearn.utils.discovery import all_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 207 docstrings\n",
      "Here is a sample of ARDRegression \n",
      "\n",
      "Bayesian ARD regression.\n",
      "\n",
      "Fit the weights of a regression model, using an ARD prior. The weights of\n",
      "the regression model are assumed to be in Gaussian distributions.\n",
      "Also estimate the parameters lambd\n"
     ]
    }
   ],
   "source": [
    "estimators = all_estimators()\n",
    "\n",
    "docstrings = []\n",
    "class_names = []\n",
    "\n",
    "for name, estimator in estimators:\n",
    "    # Check if it's actually an estimator (has fit method)\n",
    "    if hasattr(estimator, 'fit') and inspect.isclass(estimator):\n",
    "        doc = estimator.__doc__\n",
    "        if doc is not None and len(doc.strip()) > 0:\n",
    "            docstrings.append(doc)\n",
    "            class_names.append(name)\n",
    "            \n",
    "print(f\"Extracted {len(docstrings)} docstrings\")\n",
    "print(f\"Here is a sample of {class_names[0]} \\n\")\n",
    "print(docstrings[0][0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from/to: embedding_model\n",
      "Processed 64/207 documents...\n",
      "Created 128-dimensional embeddings for 207 documents\n",
      "Processing took 0.45 seconds (460.04 docs/sec)\n"
     ]
    }
   ],
   "source": [
    "# Create the embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os \n",
    "import time\n",
    "\n",
    "\n",
    "MODEL_DIR = \"embedding_model\"\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "print(f\"Loading model from/to: {MODEL_DIR}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_DIR)\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=MODEL_DIR)\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element contains token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Process in batches to avoid memory issues\n",
    "batch_size = 64  # Larger batch size for speed\n",
    "embeddings_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(docstrings), batch_size):\n",
    "    batch = docstrings[i:i+batch_size]\n",
    "    \n",
    "    # Tokenize\n",
    "    encoded_input = tokenizer(batch, padding=True, truncation=True, \n",
    "                             max_length=256, return_tensors='pt')  # Reduced max_length for speed\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    # Apply mean pooling\n",
    "    batch_embeddings = mean_pooling(model_output, encoded_input['attention_mask']).numpy()\n",
    "    embeddings_list.append(batch_embeddings)\n",
    "    \n",
    "    # Show progress sparingly for speed\n",
    "    if (i // batch_size) % 20 == 0:\n",
    "        print(f\"Processed {i+len(batch)}/{len(docstrings)} documents...\")\n",
    "\n",
    "# Concatenate all batches\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Created {embeddings.shape[1]}-dimensional embeddings for {len(docstrings)} documents\")\n",
    "print(f\"Processing took {elapsed_time:.2f} seconds ({len(docstrings)/elapsed_time:.2f} docs/sec)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with 207 documents\n"
     ]
    }
   ],
   "source": [
    "# Create the FAISS Index\n",
    "INDEX_PATH = \"sklearn_docs.index\"\n",
    "d = embeddings.shape[1]\n",
    "    \n",
    "# Create a Flat index (exact search)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings.astype(np.float32))\n",
    "faiss.write_index(index, INDEX_PATH )\n",
    "print(f\"Created index with {index.ntotal} documents\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 results for query: 'clustering algorithm for large datasets'\n",
      "1. OPTICS (Distance: 37.9485)\n",
      "   Estimate clustering structure from vector array.\n",
      "\n",
      "OPTICS (Ordering Points To Identify the Clustering Structure), closely\n",
      "related to DBSCAN, finds core...\n",
      "\n",
      "2. EllipticEnvelope (Distance: 38.2346)\n",
      "   An object for detecting outliers in a Gaussian distributed dataset.\n",
      "\n",
      "Read more in the :ref:`User Guide <outlier_detection>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "st...\n",
      "\n",
      "3. IsolationForest (Distance: 38.2359)\n",
      "   \n",
      "Isolation Forest Algorithm.\n",
      "\n",
      "Return the anomaly score of each sample using the IsolationForest algorithm\n",
      "\n",
      "The IsolationForest 'isolates' observations...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"clustering algorithm for large datasets\"\n",
    "\n",
    "encoded_input = tokenizer([query], padding=True, truncation=True, \n",
    "                             max_length=256, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    \n",
    "token_embeddings = model_output[0]\n",
    "input_mask_expanded = encoded_input['attention_mask'].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "query_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "query_embedding = query_embedding.numpy()\n",
    "\n",
    "k = 3  # Return top 3 results\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "print(f\"\\nTop {k} results for query: '{query}'\")\n",
    "for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "    print(f\"{i+1}. {class_names[idx]} (Distance: {distance:.4f})\")\n",
    "    print(f\"   {docstrings[idx][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
